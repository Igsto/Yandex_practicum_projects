{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5074419e",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a206710",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca360cc",
   "metadata": {},
   "source": [
    "Цель исследования: создать модель машинного обучения, которая сможет определить негативные комментарии на основе текста комментария."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c0dda8",
   "metadata": {},
   "source": [
    "План:\n",
    "1. Загрузка данных\n",
    "2. Обучение моделей\n",
    "3. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e377a8d",
   "metadata": {},
   "source": [
    "Перед началом работы загрузим нужные библиотеки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deff97bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3484c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mlxtend -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42588837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandarallel -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d207ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92407e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas jupyter pandarallel requests tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f40b97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aaf0000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\igsto\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    }
   ],
   "source": [
    "#pandas для работы с датафреймами\n",
    "import pandas as pd\n",
    "#математические библиотеки\n",
    "import math\n",
    "import numpy as np\n",
    "#графики\n",
    "from scipy import stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#работа с текстом\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "#\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from pymystem3 import Mystem\n",
    "import re\n",
    "import torch\n",
    "import transformers\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#undersampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as Pipeline_imb\n",
    "#\n",
    "from pandarallel import pandarallel\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress\")\n",
    "pandarallel.initialize(progress_bar = True)\n",
    "from ipywidgets import interactive\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298487f0",
   "metadata": {},
   "source": [
    "## 1. Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1787874",
   "metadata": {},
   "source": [
    "Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a5ce8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('/datasets/toxic_comments.csv') \n",
    "except:\n",
    "    try:\n",
    "        df = pd.read_csv('D://YandexPracticum//data//toxic_comments.csv') \n",
    "    except:\n",
    "        df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe8c020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c71ccc",
   "metadata": {},
   "source": [
    "Удалим столбец Unnamed: 0 так как столбец просто дублирует индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "764ebdc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b89181",
   "metadata": {},
   "source": [
    "## 2. Векторизация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a04fc",
   "metadata": {},
   "source": [
    "### 2.1 Лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01079ba",
   "metadata": {},
   "source": [
    "Зададим стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b64a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6fd94",
   "metadata": {},
   "source": [
    "Далее необходимо избавиться от лишних символов вроде переноса строки, кавычек и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660643f0",
   "metadata": {},
   "source": [
    "Заменим символы на пробелы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d8fb6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_text'] = df['text'].apply(lambda sentence: re.sub(r\"[^a-z\\']\", ' ', sentence.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3fb698",
   "metadata": {},
   "source": [
    "Зададим функцию для определение части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "222ed646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ffce4",
   "metadata": {},
   "source": [
    "Найдём часть речи для каждого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf55052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|███████████████████████████████████████████████████████████████| 159292/159292 [12:26<00:00, 213.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         [(explanation, NN), (why, WRB), (the, DT), (ed...\n",
       "1         [(d'aww, NN), (he, PRP), (matches, VBZ), (this...\n",
       "2         [(hey, NN), (man, NN), (i, NN), ('m, VBP), (re...\n",
       "3         [(more, RBR), (i, NNS), (ca, MD), (n't, RB), (...\n",
       "4         [(you, PRP), (sir, VBP), (are, VBP), (my, PRP$...\n",
       "                                ...                        \n",
       "159287    [(and, CC), (for, IN), (the, DT), (second, JJ)...\n",
       "159288    [(you, PRP), (should, MD), (be, VB), (ashamed,...\n",
       "159289    [(spitzer, NN), (umm, JJ), (theres, VBZ), (no,...\n",
       "159290    [(and, CC), (it, PRP), (looks, VBZ), (like, IN...\n",
       "159291    [(and, CC), (i, VB), (really, RB), (do, VBP), ...\n",
       "Name: token_text, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['token_text'] = df['token_text'].progress_apply(lambda sentence: nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "df['token_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccc99e",
   "metadata": {},
   "source": [
    "Преобразуем часть речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6730a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_text'] = df['token_text'].apply(\n",
    "    lambda sentence: list(map(lambda x: (x[0], pos_tagger(x[1])), sentence))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836c6bef",
   "metadata": {},
   "source": [
    "Лемматизируем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f4e068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress: 100%|██████████████████████████████████████████████████████████████| 159292/159292 [00:46<00:00, 3454.43it/s]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['token_text'] = df['token_text'].progress_apply(\n",
    "    lambda sentence: [(lambda word, tag: word if tag is None else lemmatizer.lemmatize(word, tag))(word, tag)\n",
    "     for word, tag in sentence]\n",
    ")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f8ee0",
   "metadata": {},
   "source": [
    "Удалим стопслова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0195d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_text'] = df['token_text'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b5e92eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, make, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[d'aww, match, background, colour, 'm, seeming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, 'm, really, try, edit, war, 's, guy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[ca, n't, make, real, suggestion, improvement,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page, 's]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic  \\\n",
       "0  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  D'aww! He matches this background colour I'm s...      0   \n",
       "2  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "                                          token_text  \n",
       "0  [explanation, edits, make, username, hardcore,...  \n",
       "1  [d'aww, match, background, colour, 'm, seeming...  \n",
       "2  [hey, man, 'm, really, try, edit, war, 's, guy...  \n",
       "3  [ca, n't, make, real, suggestion, improvement,...  \n",
       "4            [sir, hero, chance, remember, page, 's]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68679c8e",
   "metadata": {},
   "source": [
    "### 2.2 Векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc216d95",
   "metadata": {},
   "source": [
    "Посмотрим на количество токсичных и нетоксичных комментариев."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e70203eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    143106\n",
       "1     16186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022328b5",
   "metadata": {},
   "source": [
    "Нетоксичных комментариев примерно в 10 раз больше чем токсичных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6c375c",
   "metadata": {},
   "source": [
    "Далее необходимо посчитать tf-idf для каждого слова. Размер датафрейма довольно большой, а слов ещё больше. Необходимо как-то уменьшить количество слов. Можно найти частоту появления каждого слова и выбрать 2500 слов что встречаются чаще всего, после чего рассчитать tf-idf. Однако так как токсичных комментариев меньше необходимо найти слова, которые свойствены только токсичным комментариям и добавить эти слова к самым частоупотребляемым."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c9831b",
   "metadata": {},
   "source": [
    "Посчитаем слова для каждой группы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7116347",
   "metadata": {},
   "source": [
    "Сначала посчитаем слова для нетоксичных отзывов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98e61131",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_toxic_words = []\n",
    "\n",
    "for sentence in df.loc[df['toxic'] == 0, 'token_text']:\n",
    "    for word in sentence:\n",
    "        not_toxic_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ead6ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_toxic_words = nltk.FreqDist(not_toxic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2c190",
   "metadata": {},
   "source": [
    "Найдём количество слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "834063c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153130"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_toxic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e476f8",
   "metadata": {},
   "source": [
    "Создадим список со словами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "741974b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_toxic_words = list(not_toxic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25a1d1",
   "metadata": {},
   "source": [
    "Проделаем аналогичную операцию для токсичной группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6e48806",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_words = []\n",
    "\n",
    "for sentence in df.loc[df['toxic'] == 1, 'token_text']:\n",
    "    for word in sentence:\n",
    "        toxic_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "222d2056",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_words = nltk.FreqDist(toxic_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fcca8bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29431"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe8a952",
   "metadata": {},
   "source": [
    "Краткий осмотр показывает, что некоторые слова встречаются как в токсичной группе, так и в нетоксичной, при этом нельзя исключать обратной ситуации. Здравый смысл подсказывает, что токсичных слов в нетоксичной группе будет мало, поэтому возьмём первые 2500 слов из нетоксичной группы, а потом уберём эти слова из токсичной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a199bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_toxic_words = not_toxic_words[:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b4cadd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck',\n",
       " \"n't\",\n",
       " 'suck',\n",
       " \"'s\",\n",
       " 'go',\n",
       " 'like',\n",
       " 'wikipedia',\n",
       " 'shit',\n",
       " 'nigger',\n",
       " 'get',\n",
       " 'u',\n",
       " 'page',\n",
       " 'know',\n",
       " 'hate',\n",
       " 'faggot',\n",
       " \"'re\",\n",
       " 'bitch',\n",
       " 'gay',\n",
       " 'make',\n",
       " 'die',\n",
       " 'article',\n",
       " 'people',\n",
       " 'block',\n",
       " 'say',\n",
       " 'fat',\n",
       " 'moron',\n",
       " 'talk',\n",
       " 'cunt',\n",
       " 'user',\n",
       " 'one',\n",
       " 'think',\n",
       " 'edit',\n",
       " 'hi',\n",
       " \"'m\",\n",
       " 'jew',\n",
       " 'stop',\n",
       " 'want',\n",
       " 'stupid',\n",
       " 'dick',\n",
       " 'wiki',\n",
       " 'time',\n",
       " 'pig',\n",
       " 'cock',\n",
       " 'see',\n",
       " 'would',\n",
       " 'take',\n",
       " 'penis',\n",
       " \"'\",\n",
       " 'life',\n",
       " 'delete',\n",
       " 'fucking',\n",
       " 'right',\n",
       " 'bullshit',\n",
       " 'idiot',\n",
       " 'good',\n",
       " 'asshole',\n",
       " 'even',\n",
       " 'give',\n",
       " 'come',\n",
       " 'use',\n",
       " 'fag',\n",
       " 'dont',\n",
       " 'wanker',\n",
       " 'bad',\n",
       " 'please',\n",
       " 'ban',\n",
       " 'ball',\n",
       " 'try',\n",
       " 'tell',\n",
       " 'bark',\n",
       " 'vandalism',\n",
       " 'well',\n",
       " 'thing',\n",
       " 'sex',\n",
       " 'kill',\n",
       " 'love',\n",
       " 'need',\n",
       " 'little',\n",
       " 'call',\n",
       " 'really',\n",
       " 'piece',\n",
       " 'look',\n",
       " 'care',\n",
       " 'eat',\n",
       " 'ass',\n",
       " 'nipple',\n",
       " 'also',\n",
       " 'hey',\n",
       " 'hell',\n",
       " 'ca',\n",
       " 'bastard',\n",
       " 'way',\n",
       " 'aid',\n",
       " 'back',\n",
       " 'write',\n",
       " 'fucker',\n",
       " 'dickhead',\n",
       " 'guy',\n",
       " 'keep',\n",
       " 'big',\n",
       " 'post',\n",
       " 'admin',\n",
       " 'remove',\n",
       " 'nothing',\n",
       " 'leave',\n",
       " \"'ll\",\n",
       " \"'ve\",\n",
       " 'damn',\n",
       " 'shut',\n",
       " 'comment',\n",
       " 'old',\n",
       " 'loser',\n",
       " 'work',\n",
       " 'attack',\n",
       " 'read',\n",
       " 'edits',\n",
       " 'name',\n",
       " 'fact',\n",
       " 'mean',\n",
       " 'freedom',\n",
       " 'never',\n",
       " 'person',\n",
       " 'rule',\n",
       " 'fucksex',\n",
       " 'yourselfgo',\n",
       " 'add',\n",
       " 'rape',\n",
       " 'ever',\n",
       " 'still',\n",
       " 'f',\n",
       " 'day',\n",
       " 'man',\n",
       " 'real',\n",
       " 'mother',\n",
       " 'change',\n",
       " 'homo',\n",
       " 'source',\n",
       " 'revert',\n",
       " 'god',\n",
       " 'let',\n",
       " 'information',\n",
       " 'im',\n",
       " 'something',\n",
       " 'anything',\n",
       " 'huge',\n",
       " 'much',\n",
       " 'put',\n",
       " 'oh',\n",
       " 'editor',\n",
       " 'someone',\n",
       " 'cocksucker',\n",
       " 'find',\n",
       " 'us',\n",
       " 'dog',\n",
       " 'year',\n",
       " 'world',\n",
       " 'super',\n",
       " 'word',\n",
       " 'hitler',\n",
       " 'many',\n",
       " 'could',\n",
       " 'poop',\n",
       " 'personal',\n",
       " 'wrong',\n",
       " 'must',\n",
       " 'buttsecks',\n",
       " 'first',\n",
       " 'site',\n",
       " 'mothjer',\n",
       " 'live',\n",
       " 'show',\n",
       " 'pussy',\n",
       " 'fggt',\n",
       " 'admins',\n",
       " 'reason',\n",
       " 'noobs',\n",
       " 'every',\n",
       " 'bastered',\n",
       " 'fool',\n",
       " 'pro',\n",
       " 'lick',\n",
       " 'new',\n",
       " 'bush',\n",
       " 'dumb',\n",
       " 'lie',\n",
       " 'assad',\n",
       " 'place',\n",
       " 'point',\n",
       " 'ip',\n",
       " 'link',\n",
       " 'since',\n",
       " 'crap',\n",
       " 'anal',\n",
       " 'another',\n",
       " 'hope',\n",
       " 'seem',\n",
       " 'around',\n",
       " 'message',\n",
       " 'anyone',\n",
       " 'heil',\n",
       " 'power',\n",
       " 'mexican',\n",
       " 'actually',\n",
       " 'problem',\n",
       " 'pathetic',\n",
       " 'boob',\n",
       " 'thanks',\n",
       " 'na',\n",
       " 'ask',\n",
       " 'racist',\n",
       " 'administrator',\n",
       " 'account',\n",
       " 'last',\n",
       " 'small',\n",
       " 'lol',\n",
       " 'state',\n",
       " 'history',\n",
       " 'fuckin',\n",
       " 'feel',\n",
       " 'hanibal',\n",
       " 'help',\n",
       " 'offfuck',\n",
       " 'else',\n",
       " 'long',\n",
       " 'ur',\n",
       " 'head',\n",
       " 'believe',\n",
       " 'abuse',\n",
       " 'yet',\n",
       " 'warning',\n",
       " 'fan',\n",
       " 'report',\n",
       " 'fart',\n",
       " 'may',\n",
       " 'nazi',\n",
       " 'nice',\n",
       " 'american',\n",
       " 'understand',\n",
       " 'image',\n",
       " 'claim',\n",
       " 'ha',\n",
       " 'niggas',\n",
       " 'useless',\n",
       " 'away',\n",
       " 'alone',\n",
       " 'sexsex',\n",
       " 'without',\n",
       " 'sexual',\n",
       " 'fire',\n",
       " 'yeah',\n",
       " \"'fuck\",\n",
       " 'two',\n",
       " 'start',\n",
       " 'maybe',\n",
       " 'whore',\n",
       " 'enough',\n",
       " 'sorry',\n",
       " 'wo',\n",
       " 'continue',\n",
       " 'jewish',\n",
       " 'sure',\n",
       " 'notrhbysouthbanof',\n",
       " 'friend',\n",
       " 'face',\n",
       " 'free',\n",
       " 'retard',\n",
       " 'wp',\n",
       " 'opinion',\n",
       " 'war',\n",
       " 'j',\n",
       " 'act',\n",
       " 'http',\n",
       " 'c',\n",
       " 'others',\n",
       " 'smell',\n",
       " 'play',\n",
       " 'cocksucking',\n",
       " 'yes',\n",
       " 'org',\n",
       " 'oxymoron',\n",
       " 'p',\n",
       " 'watch',\n",
       " 'computer',\n",
       " 'truth',\n",
       " 'piss',\n",
       " 'troll',\n",
       " 'seriously',\n",
       " 'consider',\n",
       " 'r',\n",
       " 'prick',\n",
       " 'criminalwar',\n",
       " 'self',\n",
       " 'bot',\n",
       " 'bunksteve',\n",
       " 'everyone',\n",
       " 'part',\n",
       " 'utc',\n",
       " 'probably',\n",
       " 'kind',\n",
       " 'china',\n",
       " 'question',\n",
       " 'unblock',\n",
       " 'instead',\n",
       " 'great',\n",
       " 'anti',\n",
       " 'black',\n",
       " 'true',\n",
       " 'mind',\n",
       " 'already',\n",
       " 'chester',\n",
       " 'whatever',\n",
       " 'white',\n",
       " 'child',\n",
       " 'bum',\n",
       " \"'d\",\n",
       " 'mitt',\n",
       " 'romney',\n",
       " 'marcolfuck',\n",
       " 'vandalize',\n",
       " 'gon',\n",
       " 'en',\n",
       " 'retarded',\n",
       " 'game',\n",
       " 'vandal',\n",
       " 'create',\n",
       " 'guess',\n",
       " 'reference',\n",
       " 'bloody',\n",
       " 'discussion',\n",
       " 'idea',\n",
       " 'happen',\n",
       " 'picture',\n",
       " 'stuff',\n",
       " 'list',\n",
       " 'dirty',\n",
       " 'internet',\n",
       " 'website',\n",
       " 'terrorist',\n",
       " 'thank',\n",
       " 'jerk',\n",
       " 'arse',\n",
       " 'com',\n",
       " 'homeland',\n",
       " 'tommy',\n",
       " 'section',\n",
       " 'job',\n",
       " 'joke',\n",
       " 'encyclopedia',\n",
       " 'dude',\n",
       " 'cheese',\n",
       " 'fun',\n",
       " 'n',\n",
       " 'fack',\n",
       " 'insult',\n",
       " 'matter',\n",
       " 'allow',\n",
       " 'cause',\n",
       " 'sick',\n",
       " 'always',\n",
       " 'vomit',\n",
       " 'kid',\n",
       " 'though',\n",
       " 'boy',\n",
       " 'hard',\n",
       " 'everything',\n",
       " 'might',\n",
       " 'wale',\n",
       " 'securityfuck',\n",
       " 'cuntbag',\n",
       " 'cougar',\n",
       " 'full',\n",
       " 'proof',\n",
       " 'spanish',\n",
       " 'view',\n",
       " 'cant',\n",
       " 'clearly',\n",
       " 'waste',\n",
       " 'deal',\n",
       " 'enjoy',\n",
       " 'b',\n",
       " 'end',\n",
       " 'lot',\n",
       " 'obviously',\n",
       " 'far',\n",
       " 'death',\n",
       " 'youbollocks',\n",
       " 'run',\n",
       " 'ok',\n",
       " 'contribution',\n",
       " 'stay',\n",
       " 'ignorant',\n",
       " 'woman',\n",
       " 'either',\n",
       " 'whole',\n",
       " 'ya',\n",
       " 'threat',\n",
       " 'case',\n",
       " 'faggots',\n",
       " 'veggietales',\n",
       " 'country',\n",
       " 'sad',\n",
       " 'k',\n",
       " 'atheist',\n",
       " 'e',\n",
       " 'language',\n",
       " 'egg',\n",
       " 'edgar',\n",
       " 'son',\n",
       " 'ancestryfuck',\n",
       " 'ugly',\n",
       " 'hello',\n",
       " 'support',\n",
       " 'english',\n",
       " 'mention',\n",
       " 'policy',\n",
       " 'action',\n",
       " 'million',\n",
       " 'issue',\n",
       " 'prove',\n",
       " 'bunch',\n",
       " 'least',\n",
       " 'bias',\n",
       " 'chicken',\n",
       " 'vagina',\n",
       " 'content',\n",
       " 'warn',\n",
       " 'robert',\n",
       " 'respect',\n",
       " 'nigga',\n",
       " 'censor',\n",
       " 'delanoy',\n",
       " 'nhrhs',\n",
       " 'check',\n",
       " 'anthony',\n",
       " 'anyway',\n",
       " 'hand',\n",
       " 'agree',\n",
       " 'eye',\n",
       " 'next',\n",
       " 'licker',\n",
       " 'drink',\n",
       " 'hear',\n",
       " 'address',\n",
       " 'less',\n",
       " 'include',\n",
       " 'sock',\n",
       " 'repeat',\n",
       " 'thats',\n",
       " 'girl',\n",
       " 'mum',\n",
       " 'correct',\n",
       " 'become',\n",
       " 'motherfucker',\n",
       " 'liar',\n",
       " 'bradbury',\n",
       " 'ullmann',\n",
       " 'bother',\n",
       " 'ruin',\n",
       " 'school',\n",
       " 'stick',\n",
       " 'book',\n",
       " 'bag',\n",
       " 'dead',\n",
       " 'bollock',\n",
       " 'shitfuck',\n",
       " 'douche',\n",
       " 'lose',\n",
       " 'mouth',\n",
       " 'bit',\n",
       " 'centraliststupid',\n",
       " 'second',\n",
       " 'follow',\n",
       " 'dare',\n",
       " 'jim',\n",
       " 'listen',\n",
       " 'accuse',\n",
       " 'realize',\n",
       " 'sign',\n",
       " 'clear',\n",
       " 'ahead',\n",
       " 'learn',\n",
       " 'suggest',\n",
       " 'simply',\n",
       " 'king',\n",
       " 'happy',\n",
       " 'week',\n",
       " 'rather',\n",
       " 'high',\n",
       " 'murder',\n",
       " 'wow',\n",
       " 'fail',\n",
       " 'speak',\n",
       " 'lmao',\n",
       " 'quite',\n",
       " 'mongo',\n",
       " 'course',\n",
       " 'scum',\n",
       " 'deserve',\n",
       " 'explain',\n",
       " 'fight',\n",
       " 'rice',\n",
       " 'sit',\n",
       " 'best',\n",
       " 'quit',\n",
       " 'nonsense',\n",
       " 'non',\n",
       " 'false',\n",
       " 'ignore',\n",
       " 'human',\n",
       " 'wait',\n",
       " 'wish',\n",
       " 'completely',\n",
       " 'grow',\n",
       " 'sense',\n",
       " 'send',\n",
       " 'note',\n",
       " 'george',\n",
       " 'along',\n",
       " 'different',\n",
       " 'dumbass',\n",
       " 'ah',\n",
       " 'answer',\n",
       " 'hour',\n",
       " 'mom',\n",
       " 'deletion',\n",
       " 'spend',\n",
       " 'shall',\n",
       " 'term',\n",
       " 'brain',\n",
       " 'however',\n",
       " 'bollocks',\n",
       " 'forever',\n",
       " 'reply',\n",
       " 'funny',\n",
       " 'regard',\n",
       " 'bring',\n",
       " 'evidence',\n",
       " 'provide',\n",
       " 'twat',\n",
       " 'notice',\n",
       " 'bully',\n",
       " 'win',\n",
       " 'mr',\n",
       " 'etc',\n",
       " 'supertr',\n",
       " 'coward',\n",
       " 'threaten',\n",
       " 'pay',\n",
       " 'decide',\n",
       " 'nerd',\n",
       " 'number',\n",
       " 'contribute',\n",
       " 'wikipedians',\n",
       " 'house',\n",
       " 'yo',\n",
       " 'stand',\n",
       " 'bleachanhero',\n",
       " 'attempt',\n",
       " 'pov',\n",
       " 'nobody',\n",
       " 'family',\n",
       " 'expect',\n",
       " 'er',\n",
       " 'band',\n",
       " 'bbb',\n",
       " 'aidsaids',\n",
       " 'turn',\n",
       " 'blank',\n",
       " 'info',\n",
       " 'men',\n",
       " 'example',\n",
       " 'statement',\n",
       " 'due',\n",
       " 'wonder',\n",
       " 'side',\n",
       " 'soon',\n",
       " 'poor',\n",
       " 'butt',\n",
       " 'shithead',\n",
       " 'john',\n",
       " 'complete',\n",
       " 'reliable',\n",
       " 'important',\n",
       " 'request',\n",
       " 'hairy',\n",
       " 'bitchmattythewhite',\n",
       " 'forget',\n",
       " 'sort',\n",
       " 'cite',\n",
       " 'destroy',\n",
       " 'quote',\n",
       " 'text',\n",
       " 'subject',\n",
       " 'base',\n",
       " 'worthless',\n",
       " 'argument',\n",
       " 'g',\n",
       " 'freak',\n",
       " 'homosexual',\n",
       " 'business',\n",
       " 'group',\n",
       " 'arrest',\n",
       " 'move',\n",
       " 'ill',\n",
       " 'google',\n",
       " 'san',\n",
       " 'jones',\n",
       " 'daedalus',\n",
       " 'especially',\n",
       " 'muslim',\n",
       " 'burn',\n",
       " 'protect',\n",
       " 'nl',\n",
       " 'discuss',\n",
       " 'ridiculous',\n",
       " 'serious',\n",
       " 'www',\n",
       " 'sound',\n",
       " 'shithole',\n",
       " 'entire',\n",
       " 'pretty',\n",
       " 'hole',\n",
       " 'several',\n",
       " 'censorship',\n",
       " 'home',\n",
       " 'haahhahahah',\n",
       " 'yaaa',\n",
       " 'yaaaa',\n",
       " 'screw',\n",
       " 'dear',\n",
       " 'break',\n",
       " 'wtf',\n",
       " 'appear',\n",
       " 'biznitch',\n",
       " 'single',\n",
       " 'attention',\n",
       " 'laugh',\n",
       " 'dipshit',\n",
       " 'evil',\n",
       " 'rest',\n",
       " 'meet',\n",
       " 'three',\n",
       " 'california',\n",
       " 'fix',\n",
       " 'control',\n",
       " 'minority',\n",
       " 'jesus',\n",
       " 'line',\n",
       " 'project',\n",
       " 'perhaps',\n",
       " 'exist',\n",
       " 'harass',\n",
       " 'propaganda',\n",
       " 'mod',\n",
       " 'chocobos',\n",
       " 'shitty',\n",
       " 'hat',\n",
       " 'cut',\n",
       " 'cry',\n",
       " 'close',\n",
       " 'baby',\n",
       " 'communism',\n",
       " 'suppose',\n",
       " 'simple',\n",
       " 'excuse',\n",
       " 'okay',\n",
       " 'member',\n",
       " 'month',\n",
       " 'anymore',\n",
       " 'wan',\n",
       " 'news',\n",
       " 'topic',\n",
       " 'research',\n",
       " 'able',\n",
       " 'cool',\n",
       " 'six',\n",
       " 'money',\n",
       " 'bet',\n",
       " 'ck',\n",
       " 'respond',\n",
       " 'accept',\n",
       " 'diego',\n",
       " 'shoot',\n",
       " 'exactly',\n",
       " 'force',\n",
       " 'vista',\n",
       " 'salt',\n",
       " 'remember',\n",
       " 'liberal',\n",
       " 'standard',\n",
       " 'order',\n",
       " 'ago',\n",
       " 'fine',\n",
       " 'fascist',\n",
       " 'violate',\n",
       " 'obvious',\n",
       " 'l',\n",
       " 'knob',\n",
       " 'hit',\n",
       " 'steal',\n",
       " 'parent',\n",
       " 'cuntliz',\n",
       " 'today',\n",
       " 'weak',\n",
       " 'defend',\n",
       " 'hide',\n",
       " 'together',\n",
       " 'past',\n",
       " 'type',\n",
       " 'boo',\n",
       " 'speech',\n",
       " \"''\",\n",
       " 'refer',\n",
       " 'chula',\n",
       " 'system',\n",
       " 'title',\n",
       " 'response',\n",
       " 'hurt',\n",
       " 'faith',\n",
       " 'puppet',\n",
       " 'throw',\n",
       " 'half',\n",
       " 'totally',\n",
       " 'open',\n",
       " 'silly',\n",
       " 'community',\n",
       " 'kiss',\n",
       " 'christian',\n",
       " 'shame',\n",
       " 'phuq',\n",
       " 'pennnis',\n",
       " 'pneis',\n",
       " 'pensnsnniensnsn',\n",
       " 'admit',\n",
       " 'disgust',\n",
       " 'behind',\n",
       " 'accusation',\n",
       " 'x',\n",
       " 'di',\n",
       " 'greek',\n",
       " 'kick',\n",
       " 'current',\n",
       " 'apparently',\n",
       " 'animal',\n",
       " 'communist',\n",
       " 'whether',\n",
       " 'song',\n",
       " 'slap',\n",
       " 'uuuuuu',\n",
       " 'behavior',\n",
       " 'total',\n",
       " 'mistake',\n",
       " 'original',\n",
       " 'rude',\n",
       " 'hoo',\n",
       " 'fffff',\n",
       " 'unsigned',\n",
       " 'eats',\n",
       " 'itsuck',\n",
       " 'hold',\n",
       " 'absolutely',\n",
       " 'common',\n",
       " 'entry',\n",
       " 'night',\n",
       " 'vandalise',\n",
       " 'blow',\n",
       " 'mine',\n",
       " 'argue',\n",
       " 'bear',\n",
       " 'political',\n",
       " 'public',\n",
       " 'chink',\n",
       " 'neiln',\n",
       " 'arrogant',\n",
       " 'monkey',\n",
       " 'metal',\n",
       " 'lifetime',\n",
       " 'rvv',\n",
       " 'kk',\n",
       " 'uu',\n",
       " 'miss',\n",
       " 'fit',\n",
       " 'assume',\n",
       " 'pick',\n",
       " 'rap',\n",
       " 'notable',\n",
       " 'cccccc',\n",
       " 'kkkkkk',\n",
       " 'low',\n",
       " 'fake',\n",
       " 'mess',\n",
       " 'douchebag',\n",
       " 'author',\n",
       " 'blah',\n",
       " 'law',\n",
       " 'cody',\n",
       " 'outside',\n",
       " 'asian',\n",
       " 'push',\n",
       " 'award',\n",
       " 'garbage',\n",
       " 'hypocrite',\n",
       " 'lover',\n",
       " 'tv',\n",
       " 'valid',\n",
       " 'lack',\n",
       " 'annoy',\n",
       " 'almost',\n",
       " 'diff',\n",
       " 'gayfrozen',\n",
       " 'blood',\n",
       " 'city',\n",
       " 'brother',\n",
       " 'level',\n",
       " 'trip',\n",
       " 'welcome',\n",
       " 'video',\n",
       " 'thought',\n",
       " 'sockpuppet',\n",
       " 'th',\n",
       " 'donkey',\n",
       " 'w',\n",
       " 'fair',\n",
       " 'movie',\n",
       " 'possible',\n",
       " 'edie',\n",
       " 'friggen',\n",
       " 'certainly',\n",
       " 'upon',\n",
       " 'save',\n",
       " 'citation',\n",
       " 'thread',\n",
       " 'unless',\n",
       " 'gayfag',\n",
       " 'havent',\n",
       " 'form',\n",
       " 'step',\n",
       " 'shot',\n",
       " 'indian',\n",
       " 'credit',\n",
       " 'cline',\n",
       " 'sir',\n",
       " '``',\n",
       " 'knowledge',\n",
       " 'relate',\n",
       " 'haha',\n",
       " 'except',\n",
       " 'british',\n",
       " 'babywhat',\n",
       " 'cuntfranks',\n",
       " 'minute',\n",
       " 'finally',\n",
       " 'top',\n",
       " 'refuse',\n",
       " 'music',\n",
       " 'tag',\n",
       " 'difference',\n",
       " 'party',\n",
       " 'wtc',\n",
       " 'medium',\n",
       " 'review',\n",
       " 'relevant',\n",
       " 'none',\n",
       " 'favor',\n",
       " 'enigmaman',\n",
       " 'certain',\n",
       " 'material',\n",
       " 'remark',\n",
       " 'search',\n",
       " 'later',\n",
       " 'march',\n",
       " 'shannon',\n",
       " 'spread',\n",
       " 'authority',\n",
       " 'brown',\n",
       " 'concern',\n",
       " 'cover',\n",
       " 'punk',\n",
       " 'story',\n",
       " 'crazy',\n",
       " 'mad',\n",
       " 'floor',\n",
       " 'spic',\n",
       " 'shove',\n",
       " 'police',\n",
       " 'jackass',\n",
       " 'nation',\n",
       " 'complain',\n",
       " 'buddy',\n",
       " 'template',\n",
       " 'front',\n",
       " 'somebody',\n",
       " 'didnt',\n",
       " 'touch',\n",
       " 'otherwise',\n",
       " 'kike',\n",
       " 'clean',\n",
       " 'sentence',\n",
       " 'indeed',\n",
       " 'government',\n",
       " 'rot',\n",
       " 'sweet',\n",
       " 'saw',\n",
       " 'lazy',\n",
       " 'choose',\n",
       " 'drop',\n",
       " 'mate',\n",
       " 'america',\n",
       " 'race',\n",
       " 'youre',\n",
       " 'version',\n",
       " 'film',\n",
       " 'stephen',\n",
       " 'bye',\n",
       " 'within',\n",
       " 'sake',\n",
       " 'large',\n",
       " 'spell',\n",
       " 'treat',\n",
       " 'belong',\n",
       " 'horrible',\n",
       " 'idiotic',\n",
       " 'pull',\n",
       " 'wont',\n",
       " 'individual',\n",
       " 'consensus',\n",
       " 'set',\n",
       " 'drive',\n",
       " 'summary',\n",
       " 'personally',\n",
       " 'useful',\n",
       " 'civil',\n",
       " 'contact',\n",
       " 'ahahahahahahahahahahahahahahahahahahaha',\n",
       " 'jasenm',\n",
       " 'latinus',\n",
       " 'truly',\n",
       " 'photo',\n",
       " 'arab',\n",
       " 'gamaliel',\n",
       " 'criticism',\n",
       " 'lion',\n",
       " 'attitude',\n",
       " 'v',\n",
       " 'misterwiki',\n",
       " 'dive',\n",
       " 'earth',\n",
       " 'spam',\n",
       " 'religion',\n",
       " 'although',\n",
       " 'stupidity',\n",
       " 'final',\n",
       " 'ignorance',\n",
       " 'character',\n",
       " 'special',\n",
       " 'describe',\n",
       " 'huh',\n",
       " 'college',\n",
       " 'jimbo',\n",
       " 'cuz',\n",
       " 'precede',\n",
       " 'general',\n",
       " 'h',\n",
       " 'future',\n",
       " 'ai',\n",
       " 'debate',\n",
       " 'everybody',\n",
       " 'sweep',\n",
       " 'interest',\n",
       " 'rock',\n",
       " 'mann',\n",
       " 'worth',\n",
       " 'smart',\n",
       " 'cum',\n",
       " 'beat',\n",
       " 'violation',\n",
       " 'style',\n",
       " 'russian',\n",
       " 'trouble',\n",
       " 'njgw',\n",
       " 'raid',\n",
       " 'angela',\n",
       " 'fuckingabf',\n",
       " 'bongwarriorcongratualtions',\n",
       " 'jack',\n",
       " 'lame',\n",
       " 'official',\n",
       " 'forum',\n",
       " 'israel',\n",
       " 'thousand',\n",
       " 'reality',\n",
       " 'fly',\n",
       " 'improve',\n",
       " 'class',\n",
       " 'accord',\n",
       " 'sloppy',\n",
       " 'present',\n",
       " 'harassment',\n",
       " 'age',\n",
       " 'easy',\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_words = list(toxic_words)\n",
    "toxic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3bcaf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fuck',\n",
       " 'suck',\n",
       " 'shit',\n",
       " 'nigger',\n",
       " 'faggot',\n",
       " 'bitch',\n",
       " 'fat',\n",
       " 'moron',\n",
       " 'cunt',\n",
       " 'dick',\n",
       " 'pig',\n",
       " 'cock',\n",
       " 'penis',\n",
       " 'fucking',\n",
       " 'bullshit',\n",
       " 'asshole',\n",
       " 'fag',\n",
       " 'wanker',\n",
       " 'bark',\n",
       " 'ass',\n",
       " 'nipple',\n",
       " 'bastard',\n",
       " 'fucker',\n",
       " 'dickhead',\n",
       " 'shut',\n",
       " 'loser',\n",
       " 'fucksex',\n",
       " 'yourselfgo',\n",
       " 'homo',\n",
       " 'cocksucker',\n",
       " 'poop',\n",
       " 'buttsecks',\n",
       " 'mothjer',\n",
       " 'pussy',\n",
       " 'fggt',\n",
       " 'noobs',\n",
       " 'bastered',\n",
       " 'lick',\n",
       " 'dumb',\n",
       " 'assad',\n",
       " 'anal',\n",
       " 'heil',\n",
       " 'mexican',\n",
       " 'pathetic',\n",
       " 'boob',\n",
       " 'fuckin',\n",
       " 'hanibal',\n",
       " 'offfuck',\n",
       " 'ur',\n",
       " 'fart',\n",
       " 'niggas',\n",
       " 'sexsex',\n",
       " \"'fuck\",\n",
       " 'whore',\n",
       " 'notrhbysouthbanof',\n",
       " 'retard',\n",
       " 'cocksucking',\n",
       " 'oxymoron',\n",
       " 'piss',\n",
       " 'prick',\n",
       " 'criminalwar',\n",
       " 'bunksteve',\n",
       " 'chester',\n",
       " 'bum',\n",
       " 'mitt',\n",
       " 'romney',\n",
       " 'marcolfuck',\n",
       " 'retarded',\n",
       " 'bloody',\n",
       " 'dirty',\n",
       " 'jerk',\n",
       " 'arse',\n",
       " 'homeland',\n",
       " 'tommy',\n",
       " 'cheese',\n",
       " 'fack',\n",
       " 'vomit',\n",
       " 'securityfuck',\n",
       " 'cuntbag',\n",
       " 'cougar',\n",
       " 'youbollocks',\n",
       " 'faggots',\n",
       " 'veggietales',\n",
       " 'atheist',\n",
       " 'egg',\n",
       " 'edgar',\n",
       " 'ancestryfuck',\n",
       " 'ugly',\n",
       " 'vagina',\n",
       " 'nigga',\n",
       " 'delanoy',\n",
       " 'nhrhs',\n",
       " 'anthony',\n",
       " 'licker',\n",
       " 'mum',\n",
       " 'motherfucker',\n",
       " 'bradbury',\n",
       " 'ullmann',\n",
       " 'ruin',\n",
       " 'bag',\n",
       " 'bollock',\n",
       " 'shitfuck',\n",
       " 'douche',\n",
       " 'mouth',\n",
       " 'centraliststupid',\n",
       " 'lmao',\n",
       " 'mongo',\n",
       " 'scum',\n",
       " 'rice',\n",
       " 'dumbass',\n",
       " 'mom',\n",
       " 'bollocks',\n",
       " 'twat',\n",
       " 'supertr',\n",
       " 'coward',\n",
       " 'nerd',\n",
       " 'yo',\n",
       " 'bleachanhero',\n",
       " 'er',\n",
       " 'bbb',\n",
       " 'aidsaids',\n",
       " 'butt',\n",
       " 'shithead',\n",
       " 'hairy',\n",
       " 'bitchmattythewhite',\n",
       " 'worthless',\n",
       " 'freak',\n",
       " 'homosexual',\n",
       " 'daedalus',\n",
       " 'nl',\n",
       " 'shithole',\n",
       " 'hole',\n",
       " 'haahhahahah',\n",
       " 'yaaa',\n",
       " 'yaaaa',\n",
       " 'screw',\n",
       " 'wtf',\n",
       " 'biznitch',\n",
       " 'dipshit',\n",
       " 'mod',\n",
       " 'chocobos',\n",
       " 'shitty',\n",
       " 'communism',\n",
       " 'wan',\n",
       " 'ck',\n",
       " 'diego',\n",
       " 'vista',\n",
       " 'salt',\n",
       " 'fascist',\n",
       " 'knob',\n",
       " 'cuntliz',\n",
       " 'boo',\n",
       " 'chula',\n",
       " 'kiss',\n",
       " 'phuq',\n",
       " 'pennnis',\n",
       " 'pneis',\n",
       " 'pensnsnniensnsn',\n",
       " 'disgust',\n",
       " 'di',\n",
       " 'slap',\n",
       " 'uuuuuu',\n",
       " 'hoo',\n",
       " 'fffff',\n",
       " 'eats',\n",
       " 'itsuck',\n",
       " 'chink',\n",
       " 'neiln',\n",
       " 'arrogant',\n",
       " 'monkey',\n",
       " 'lifetime',\n",
       " 'rvv',\n",
       " 'kk',\n",
       " 'uu',\n",
       " 'rap',\n",
       " 'cccccc',\n",
       " 'kkkkkk',\n",
       " 'douchebag',\n",
       " 'blah',\n",
       " 'cody',\n",
       " 'garbage',\n",
       " 'hypocrite',\n",
       " 'lover',\n",
       " 'gayfrozen',\n",
       " 'donkey',\n",
       " 'edie',\n",
       " 'friggen',\n",
       " 'gayfag',\n",
       " 'havent',\n",
       " 'cline',\n",
       " 'haha',\n",
       " 'babywhat',\n",
       " 'cuntfranks',\n",
       " 'wtc',\n",
       " 'enigmaman',\n",
       " 'shannon',\n",
       " 'floor',\n",
       " 'spic',\n",
       " 'shove',\n",
       " 'jackass',\n",
       " 'buddy',\n",
       " 'kike',\n",
       " 'rot',\n",
       " 'sweet',\n",
       " 'lazy',\n",
       " 'youre',\n",
       " 'stephen',\n",
       " 'horrible',\n",
       " 'idiotic',\n",
       " 'wont',\n",
       " 'ahahahahahahahahahahahahahahahahahahaha',\n",
       " 'jasenm',\n",
       " 'latinus',\n",
       " 'gamaliel',\n",
       " 'lion',\n",
       " 'misterwiki',\n",
       " 'dive',\n",
       " 'stupidity',\n",
       " 'huh',\n",
       " 'cuz',\n",
       " 'sweep',\n",
       " 'mann',\n",
       " 'cum',\n",
       " 'njgw',\n",
       " 'raid',\n",
       " 'angela',\n",
       " 'fuckingabf',\n",
       " 'bongwarriorcongratualtions',\n",
       " 'lame',\n",
       " 'sloppy',\n",
       " 'queer',\n",
       " 'muahahahahahahahahahahahahahahahahahaha',\n",
       " 'yooo',\n",
       " 'antivman',\n",
       " 'awardhttp',\n",
       " 'oi',\n",
       " 'jforget',\n",
       " 'bestfrozen',\n",
       " 'vuvuzelas',\n",
       " 'hawkinghttp',\n",
       " 'gg',\n",
       " 'slut',\n",
       " 'sup',\n",
       " 'goodbye',\n",
       " 'motherfucking',\n",
       " 'billj',\n",
       " 'lolooolbootstoots',\n",
       " 'sannse',\n",
       " 'porn',\n",
       " 'nut',\n",
       " 'basement',\n",
       " 'nasty',\n",
       " 'semen',\n",
       " 'clown',\n",
       " 'dave',\n",
       " 'filter',\n",
       " 'chuck',\n",
       " 'trash',\n",
       " 'pedophile',\n",
       " 'rubbish',\n",
       " 'fc',\n",
       " 'disgrace',\n",
       " 'whats',\n",
       " 'dust',\n",
       " 'libtard',\n",
       " 'fuckbags',\n",
       " 'fu',\n",
       " 'hist',\n",
       " 'virgin',\n",
       " 'limped',\n",
       " 'dicked',\n",
       " 'filthy',\n",
       " 'nate',\n",
       " 'pompous',\n",
       " 'dad',\n",
       " 'nose',\n",
       " 'imma',\n",
       " 'sucker',\n",
       " 'goddamn',\n",
       " 'eh',\n",
       " 'sk',\n",
       " 'couriano',\n",
       " 'mental',\n",
       " 'kurt',\n",
       " 'fired',\n",
       " 'hellor',\n",
       " 'ing',\n",
       " 'moderator',\n",
       " 'everyday',\n",
       " 'childish',\n",
       " 'password',\n",
       " 'stink',\n",
       " 'hahaha',\n",
       " 'lunchables',\n",
       " 'girlfriend',\n",
       " 'swear',\n",
       " 'norris',\n",
       " 'finger',\n",
       " 'hatred',\n",
       " 'cancer',\n",
       " 'flame',\n",
       " 'insane',\n",
       " 'pant',\n",
       " 'curse',\n",
       " 'construct',\n",
       " 'petty',\n",
       " 'rat',\n",
       " 'utter',\n",
       " 'fraud',\n",
       " 'anus',\n",
       " 'cking',\n",
       " 'fucky',\n",
       " 'prison',\n",
       " 'awesome',\n",
       " 'ho',\n",
       " 'wit',\n",
       " 'whine',\n",
       " 'bull',\n",
       " 'mentally',\n",
       " 'nerve',\n",
       " 'smoke',\n",
       " 'bald',\n",
       " 'giant',\n",
       " 'corrupt',\n",
       " 'stalker',\n",
       " 'nazis',\n",
       " 'juicy',\n",
       " 'ggot',\n",
       " 'mccain',\n",
       " 'bore',\n",
       " 'idiots',\n",
       " 'hack',\n",
       " 'hypocritical',\n",
       " 'ego',\n",
       " 'aint',\n",
       " 'ashamed',\n",
       " 'homosexuality',\n",
       " 'sh',\n",
       " 'bro',\n",
       " 'hater',\n",
       " 'suppress',\n",
       " 'ummmmmmm',\n",
       " 'saliva',\n",
       " 'foolish',\n",
       " 'youcaltlas',\n",
       " 'immature',\n",
       " 'bite',\n",
       " 'silence',\n",
       " 'wipe',\n",
       " 'assyrian',\n",
       " 'geek',\n",
       " 'crack',\n",
       " 'jimmy',\n",
       " 'omg',\n",
       " 'hail',\n",
       " 'curps',\n",
       " 'maher',\n",
       " 'meatspin',\n",
       " 'terrible',\n",
       " 'scumbag',\n",
       " 'isnt',\n",
       " 'ear',\n",
       " 'sexy',\n",
       " 'genital',\n",
       " 'tit',\n",
       " 'scar',\n",
       " 'billcj',\n",
       " 'tiny',\n",
       " 'zionist',\n",
       " 'commie',\n",
       " 'allemande',\n",
       " 'molest',\n",
       " 'heck',\n",
       " 'behave',\n",
       " 'ive',\n",
       " 'cop',\n",
       " 'wedge',\n",
       " 'simpson',\n",
       " 'yea',\n",
       " 'shoit',\n",
       " 'moronic',\n",
       " 'fuk',\n",
       " 'imbecile',\n",
       " 'foul',\n",
       " 'pile',\n",
       " 'turd',\n",
       " 'killer',\n",
       " 'incompetent',\n",
       " 'semite',\n",
       " 'ng',\n",
       " 'suicide',\n",
       " 'revers',\n",
       " 'pervert',\n",
       " 'tear',\n",
       " 'taste',\n",
       " 'niggaz',\n",
       " 'mommy',\n",
       " 'lesson',\n",
       " 'yank',\n",
       " 'bigot',\n",
       " 'leftist',\n",
       " 'goof',\n",
       " 'violent',\n",
       " 'prostitute',\n",
       " 'arrogance',\n",
       " 'alive',\n",
       " 'loose',\n",
       " 'manipulate',\n",
       " 'rob',\n",
       " 'bout',\n",
       " 'creature',\n",
       " 'dislike',\n",
       " 'poo',\n",
       " 'dishonest',\n",
       " 'maggot',\n",
       " 'tough',\n",
       " 'jersey',\n",
       " 'ppl',\n",
       " 'truck',\n",
       " 'thick',\n",
       " 'hypocrisy',\n",
       " 'bongwarrior',\n",
       " 'fucken',\n",
       " 'kaff',\n",
       " 'dickbutt',\n",
       " 'butthead',\n",
       " 'bang',\n",
       " 'semitic',\n",
       " 'heard',\n",
       " 'dictator',\n",
       " 'illiterate',\n",
       " 'pray',\n",
       " 'mighty',\n",
       " 'pity',\n",
       " 'thug',\n",
       " 'dildo',\n",
       " 'fantasy',\n",
       " 'punish',\n",
       " 'hahahahahahahahahahahahaha',\n",
       " 'duck',\n",
       " 'ashol',\n",
       " 'malusia',\n",
       " 'bail',\n",
       " 'masturbate',\n",
       " 'fuckhead',\n",
       " 'humanity',\n",
       " 'ps',\n",
       " 'naked',\n",
       " 'gwernol',\n",
       " 'unemployed',\n",
       " 'bed',\n",
       " 'zuck',\n",
       " 'wank',\n",
       " 'cyber',\n",
       " 'inch',\n",
       " 'scream',\n",
       " 'arsehole',\n",
       " 'moonshine',\n",
       " 'wanta',\n",
       " 'beavis',\n",
       " 'jessica',\n",
       " 'tonight',\n",
       " 'vile',\n",
       " 'uneducated',\n",
       " 'disgusting',\n",
       " 'hunt',\n",
       " 'rapist',\n",
       " 'sack',\n",
       " 'ch',\n",
       " 'bigoted',\n",
       " 'dickface',\n",
       " 'ersi',\n",
       " 'toilet',\n",
       " 'emo',\n",
       " 'viva',\n",
       " 'awful',\n",
       " 'wannabe',\n",
       " 'hahahaha',\n",
       " 'rip',\n",
       " 'dat',\n",
       " 'soul',\n",
       " 'cow',\n",
       " 'knock',\n",
       " 'yall',\n",
       " 'slander',\n",
       " 'punishment',\n",
       " 'verizon',\n",
       " 'devil',\n",
       " 'chan',\n",
       " 'crappy',\n",
       " 'drunk',\n",
       " 'lesbian',\n",
       " 'virtual',\n",
       " 'aggressive',\n",
       " 'bitchmother',\n",
       " 'pedo',\n",
       " 'contra',\n",
       " 'uh',\n",
       " 'boyfriend',\n",
       " 'gut',\n",
       " 'paranoid',\n",
       " 'anger',\n",
       " 'righteous',\n",
       " 'miserable',\n",
       " 'beth',\n",
       " 'wake',\n",
       " 'tea',\n",
       " 'anime',\n",
       " 'fuckwit',\n",
       " 'shyt',\n",
       " 'pedia',\n",
       " 'idiocy',\n",
       " 'smelly',\n",
       " 'jealous',\n",
       " 'bin',\n",
       " 'sexuality',\n",
       " 'utterly',\n",
       " 'dummy',\n",
       " 'sine',\n",
       " 'legion',\n",
       " 'um',\n",
       " 'plz',\n",
       " 'robot',\n",
       " 'feces',\n",
       " 'rajput',\n",
       " 'caltlas',\n",
       " 'scrotum',\n",
       " 'filth',\n",
       " 'beg',\n",
       " 'amuse',\n",
       " 'sand',\n",
       " 'naughty',\n",
       " 'pleasure',\n",
       " 'insulting',\n",
       " 'monster',\n",
       " 'favorite',\n",
       " 'stfu',\n",
       " 'indefinitely',\n",
       " 'blocking',\n",
       " 'homophobic',\n",
       " 'spongebob',\n",
       " 'fascists',\n",
       " 'mah',\n",
       " 'slowly',\n",
       " 'pink',\n",
       " 'sitush',\n",
       " 'uhbsirtubgyihihlkjngkjbnkgjnbkjfgnbknfgjkbnkfjgnbjkfnjbkfnjbkjbnfkjnbkjnbkjnfbfkjbnknlkshubnsutybnisueynboiserubnsiunybiosubnioseubnoitybnosiubnosriutbynsoitubnosiubnsoiubni',\n",
       " 'terri',\n",
       " 'retardedyour',\n",
       " 'jpgsuck',\n",
       " 'pashtun',\n",
       " 'rebel',\n",
       " 'superior',\n",
       " 'throat',\n",
       " 'wash',\n",
       " 'punch',\n",
       " 'daddy',\n",
       " 'denial',\n",
       " 'fucktard',\n",
       " 'regularly',\n",
       " 'wat',\n",
       " 'diety',\n",
       " 'otherwords',\n",
       " 'paterson',\n",
       " 'fanatic',\n",
       " 'permanently',\n",
       " 'pole',\n",
       " 'cretin',\n",
       " 'fuckk',\n",
       " 'breath',\n",
       " 'hazel',\n",
       " 'rodeo',\n",
       " 'ea',\n",
       " 'yammer',\n",
       " 'wolf',\n",
       " 'oli',\n",
       " 'pal',\n",
       " 'kurd',\n",
       " 'genius',\n",
       " 'smear',\n",
       " 'clever',\n",
       " 'worm',\n",
       " 'dis',\n",
       " 'hungry',\n",
       " 'pimp',\n",
       " 'precious',\n",
       " 'trick',\n",
       " 'keyboard',\n",
       " 'lucky',\n",
       " 'lil',\n",
       " 'tight',\n",
       " 'ape',\n",
       " 'arbitrary',\n",
       " 'laughable',\n",
       " 'spit',\n",
       " 'fred',\n",
       " 'alert',\n",
       " 'satan',\n",
       " 'drummer',\n",
       " 'chair',\n",
       " 'fuc',\n",
       " 'clueless',\n",
       " 'blocked',\n",
       " 'headline',\n",
       " 'noob',\n",
       " 'grandma',\n",
       " 'bootstoots',\n",
       " 'asswhole',\n",
       " 'wigger',\n",
       " 'pair',\n",
       " 'pc',\n",
       " 'kinda',\n",
       " 'husband',\n",
       " 'twit',\n",
       " 'gross',\n",
       " 'buck',\n",
       " 'cousin',\n",
       " 'descent',\n",
       " 'obsess',\n",
       " 'teh',\n",
       " 'hobby',\n",
       " 'wikipeida',\n",
       " 'lulz',\n",
       " 'blatantly',\n",
       " 'better',\n",
       " 'glass',\n",
       " 'hilarious',\n",
       " 'hispanic',\n",
       " 'victory',\n",
       " 'radical',\n",
       " 'sensitive',\n",
       " 'kkk',\n",
       " 'sexist',\n",
       " 'alex',\n",
       " 'creep',\n",
       " 'hav',\n",
       " 'santa',\n",
       " 'countless',\n",
       " 'lyric',\n",
       " 'fell',\n",
       " 'sht',\n",
       " 'pakistani',\n",
       " 'monkeyman',\n",
       " 'loloool',\n",
       " 'macro',\n",
       " 'spartucused',\n",
       " 'alstair',\n",
       " 'spout',\n",
       " 'yamla',\n",
       " 'lonely',\n",
       " 'meaningless',\n",
       " 'supremacist',\n",
       " 'iq',\n",
       " 'choke',\n",
       " 'degenerate',\n",
       " 'pusher',\n",
       " 'drag',\n",
       " 'desperate',\n",
       " 'justine',\n",
       " 'painful',\n",
       " 'thinking',\n",
       " 'tibbit',\n",
       " 'dork',\n",
       " 'feed',\n",
       " 'shower',\n",
       " 'yer',\n",
       " 'crusade',\n",
       " 'stab',\n",
       " 'fame',\n",
       " 'chick',\n",
       " 'castro',\n",
       " 'propagandist',\n",
       " 'twitter',\n",
       " 'pseudo',\n",
       " 'saaaaad',\n",
       " 'tabtab',\n",
       " 'bow',\n",
       " 'goat',\n",
       " 'weed',\n",
       " 'proven',\n",
       " 'freakin',\n",
       " 'matt',\n",
       " 'dangerous',\n",
       " 'kurdish',\n",
       " 'wasnt',\n",
       " 'amaze',\n",
       " 'bogus',\n",
       " 'lecture',\n",
       " 'cowardly',\n",
       " 'dragon',\n",
       " 'leg',\n",
       " 'hes',\n",
       " 'inane',\n",
       " 'obnoxious',\n",
       " 'surprised',\n",
       " 'yep',\n",
       " 'encounter',\n",
       " 'hood',\n",
       " 'worship',\n",
       " 'lying',\n",
       " 'ian',\n",
       " 'jeff',\n",
       " 'murderer',\n",
       " 'dynamic',\n",
       " 'guest',\n",
       " 'openly',\n",
       " 'deepak',\n",
       " 'hoe',\n",
       " 'mirror',\n",
       " 'neck',\n",
       " 'meat',\n",
       " 'pen',\n",
       " 'boring',\n",
       " 'grasp',\n",
       " 'che',\n",
       " 'pirate',\n",
       " 'ou',\n",
       " 'lessheard',\n",
       " 'vanu',\n",
       " 'numeral',\n",
       " 'tosser',\n",
       " 'malicious',\n",
       " 'cheap',\n",
       " 'uncle',\n",
       " 'teenager',\n",
       " 'shock',\n",
       " 'baseball',\n",
       " 'revenge',\n",
       " 'rub',\n",
       " 'rag',\n",
       " 'bright',\n",
       " 'signal',\n",
       " 'asswipe',\n",
       " 'slavic',\n",
       " 'pronounce',\n",
       " 'succeed',\n",
       " 'nutcase',\n",
       " 'yankee',\n",
       " 'bos',\n",
       " 'suspend',\n",
       " 'arrange',\n",
       " 'tale',\n",
       " 'legit',\n",
       " 'fatuorum',\n",
       " 'johnny',\n",
       " 'redneck',\n",
       " 'strip',\n",
       " 'boi',\n",
       " 'prof',\n",
       " 'telugu',\n",
       " 'fruit',\n",
       " 'hateful',\n",
       " 'bleed',\n",
       " 'mutha',\n",
       " 'fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck',\n",
       " 'refute',\n",
       " 'jail',\n",
       " 'misery',\n",
       " 'aleem',\n",
       " 'squad',\n",
       " 'provoke',\n",
       " 'hmm',\n",
       " 'heh',\n",
       " 'cake',\n",
       " 'sucksfrozen',\n",
       " \"'homosexuals\",\n",
       " 'tw',\n",
       " 'bugger',\n",
       " 'ethical',\n",
       " 'resort',\n",
       " 'lousy',\n",
       " 'cheat',\n",
       " 'sarcastic',\n",
       " 'swallow',\n",
       " 'tired',\n",
       " 'sore',\n",
       " 'poison',\n",
       " 'mouse',\n",
       " 'path',\n",
       " 'junk',\n",
       " 'fist',\n",
       " 'mel',\n",
       " 'wonderful',\n",
       " 'chill',\n",
       " 'sheep',\n",
       " 'ufc',\n",
       " 'insignificant',\n",
       " 'malleus',\n",
       " 'pee',\n",
       " 'crush',\n",
       " 'urantia',\n",
       " 'dam',\n",
       " 'doosh',\n",
       " 'slang',\n",
       " 'publicly',\n",
       " 'delusional',\n",
       " 'aleppo',\n",
       " 'gvhy',\n",
       " 'bury',\n",
       " 'pretentious',\n",
       " 'cure',\n",
       " 'shite',\n",
       " 'ka',\n",
       " 'tongue',\n",
       " 'disturb',\n",
       " 'brainwash',\n",
       " 'playing',\n",
       " 'traitor',\n",
       " 'amazing',\n",
       " 'aussie',\n",
       " 'thumb',\n",
       " 'assassination',\n",
       " 'supressing',\n",
       " 'suckersyou',\n",
       " 'shioty',\n",
       " 'ashit',\n",
       " 'shgit',\n",
       " 'ahot',\n",
       " 'aot',\n",
       " 'shiot',\n",
       " 'shti',\n",
       " 'sot',\n",
       " 'facists',\n",
       " 'appoint',\n",
       " 'sin',\n",
       " 'empty',\n",
       " 'drown',\n",
       " 'everytime',\n",
       " 'crawl',\n",
       " 'ride',\n",
       " 'bust',\n",
       " 'knee',\n",
       " 'ancestor',\n",
       " 'cracker',\n",
       " 'crowd',\n",
       " 'boston',\n",
       " 'bash',\n",
       " 'chase',\n",
       " 'idle',\n",
       " 'weasel',\n",
       " 'grandmother',\n",
       " 'whiny',\n",
       " 'trap',\n",
       " 'sean',\n",
       " 'obsession',\n",
       " 'frustrate',\n",
       " 'warm',\n",
       " 'torture',\n",
       " 'sight',\n",
       " 'traffic',\n",
       " 'boot',\n",
       " 'mindless',\n",
       " \"'em\",\n",
       " 'breast',\n",
       " 'bound',\n",
       " 'donate',\n",
       " 'thief',\n",
       " 'myspace',\n",
       " 'wet',\n",
       " 'urge',\n",
       " 'ruler',\n",
       " 'spew',\n",
       " 'annoying',\n",
       " 'karen',\n",
       " 'brave',\n",
       " 'peice',\n",
       " 'stinky',\n",
       " 'doom',\n",
       " 'someday',\n",
       " 'bishonen',\n",
       " 'stalin',\n",
       " 'newbie',\n",
       " 'puke',\n",
       " 'ybm',\n",
       " 'obsessive',\n",
       " 'tyrant',\n",
       " 'deeply',\n",
       " 'fukin',\n",
       " 'scribble',\n",
       " 'intimidate',\n",
       " 'sponsor',\n",
       " 'hsoit',\n",
       " 'grab',\n",
       " 'horny',\n",
       " 'mature',\n",
       " 'testicle',\n",
       " 'breathe',\n",
       " 'hunjan',\n",
       " 'joker',\n",
       " 'jeremy',\n",
       " 'patience',\n",
       " 'drmies',\n",
       " 'snitch',\n",
       " 'soooo',\n",
       " 'le',\n",
       " 'ally',\n",
       " 'decency',\n",
       " 'crook',\n",
       " 'ultra',\n",
       " 'harrassing',\n",
       " 'cowboy',\n",
       " 'coffee',\n",
       " 'unwarranted',\n",
       " 'sysop',\n",
       " 'vindictive',\n",
       " 'shes',\n",
       " 'xd',\n",
       " 'chain',\n",
       " 'twist',\n",
       " 'defence',\n",
       " 'loud',\n",
       " 'fatty',\n",
       " 'airport',\n",
       " 'shoulder',\n",
       " 'bare',\n",
       " 'slur',\n",
       " 'slavery',\n",
       " 'actively',\n",
       " 'awww',\n",
       " 'bat',\n",
       " 'wrestling',\n",
       " 'cute',\n",
       " 'syndrome',\n",
       " 'comedy',\n",
       " 'purple',\n",
       " 'dahn',\n",
       " 'unfortunate',\n",
       " 'cena',\n",
       " 'orientation',\n",
       " 'discredit',\n",
       " 'dominate',\n",
       " 'nu',\n",
       " 'flap',\n",
       " 'pecker',\n",
       " 'gtfo',\n",
       " 'output',\n",
       " 'actress',\n",
       " 'marine',\n",
       " 'trace',\n",
       " 'nickname',\n",
       " 'crum',\n",
       " 'extremist',\n",
       " 'slight',\n",
       " 'deathcore',\n",
       " 'weeaboo',\n",
       " 'putin',\n",
       " 'algorithm',\n",
       " 'nope',\n",
       " 'fagget',\n",
       " 'backward',\n",
       " 'ni',\n",
       " 'bone',\n",
       " 'grave',\n",
       " 'clan',\n",
       " 'bud',\n",
       " 'narrow',\n",
       " 'crybaby',\n",
       " 'kitten',\n",
       " 'negro',\n",
       " 'scumbags',\n",
       " 'momma',\n",
       " 'bizarre',\n",
       " 'wee',\n",
       " 'facist',\n",
       " 'reverting',\n",
       " 'becuase',\n",
       " 'goon',\n",
       " 'classify',\n",
       " 'stance',\n",
       " 'retentive',\n",
       " 'falsely',\n",
       " 'freud',\n",
       " 'wine',\n",
       " 'virtually',\n",
       " 'lip',\n",
       " 'dress',\n",
       " 'courtesy',\n",
       " 'cheek',\n",
       " 'fuckface',\n",
       " 'pedantic',\n",
       " 'homework',\n",
       " 'rush',\n",
       " 'npa',\n",
       " 'quiet',\n",
       " 'democrat',\n",
       " 'brit',\n",
       " 'distort',\n",
       " 'smack',\n",
       " 'demon',\n",
       " 'scare',\n",
       " 'untrue',\n",
       " 'jus',\n",
       " 'bless',\n",
       " 'hip',\n",
       " 'forbid',\n",
       " 'eric',\n",
       " 'humor',\n",
       " 'truthful',\n",
       " 'hacker',\n",
       " 'wrestle',\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_words = [word for word in toxic_words if word not in not_toxic_words]\n",
    "toxic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8aed6b3",
   "metadata": {},
   "source": [
    "Найдём длину списка токсичных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2a7a495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26946"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(toxic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cedde6e",
   "metadata": {},
   "source": [
    "Возьмём первые 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac410660",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_words = toxic_words[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "349a66f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\",\n",
       " 'page',\n",
       " \"n't\",\n",
       " 'wikipedia',\n",
       " 'talk',\n",
       " 'use',\n",
       " 'would',\n",
       " 'one',\n",
       " 'please',\n",
       " 'edit',\n",
       " 'make',\n",
       " 'like',\n",
       " 'see',\n",
       " 'say',\n",
       " 'think',\n",
       " 'source',\n",
       " 'know',\n",
       " 'also',\n",
       " 'get',\n",
       " 'add',\n",
       " 'time',\n",
       " 'go',\n",
       " 'people',\n",
       " 'user',\n",
       " \"'m\",\n",
       " 'good',\n",
       " 'may',\n",
       " 'need',\n",
       " 'link',\n",
       " 'name',\n",
       " 'image',\n",
       " 'take',\n",
       " 'block',\n",
       " \"'\",\n",
       " 'find',\n",
       " 'delete',\n",
       " 'remove',\n",
       " 'want',\n",
       " 'well',\n",
       " 'look',\n",
       " 'work',\n",
       " 'thanks',\n",
       " 'even',\n",
       " 'could',\n",
       " 'help',\n",
       " 'list',\n",
       " 'comment',\n",
       " 'deletion',\n",
       " 'change',\n",
       " 'information',\n",
       " 'section',\n",
       " 'question',\n",
       " 'way',\n",
       " \"'ve\",\n",
       " 'point',\n",
       " 'write',\n",
       " 'editor',\n",
       " 'give',\n",
       " 'wp',\n",
       " 'first',\n",
       " 'try',\n",
       " 'new',\n",
       " 'thank',\n",
       " 'thing',\n",
       " 'fact',\n",
       " 'seem',\n",
       " 'state',\n",
       " 'discussion',\n",
       " 'reference',\n",
       " 'read',\n",
       " 'place',\n",
       " 'ask',\n",
       " 'many',\n",
       " 'right',\n",
       " 'much',\n",
       " 'revert',\n",
       " 'edits',\n",
       " 'include',\n",
       " 'create',\n",
       " 'tag',\n",
       " 'mean',\n",
       " 'really',\n",
       " 'since',\n",
       " 'note',\n",
       " 'come',\n",
       " 'reason',\n",
       " 'policy',\n",
       " 'issue',\n",
       " 'content',\n",
       " \"'re\",\n",
       " 'two',\n",
       " 'show',\n",
       " 'someone',\n",
       " 'back',\n",
       " 'call',\n",
       " 'word',\n",
       " 'year',\n",
       " 'post',\n",
       " 'case',\n",
       " 'still',\n",
       " 'consider',\n",
       " 'leave',\n",
       " 'mention',\n",
       " \"'ll\",\n",
       " 'put',\n",
       " 'claim',\n",
       " 'http',\n",
       " 'something',\n",
       " 'without',\n",
       " 'history',\n",
       " 'keep',\n",
       " 'problem',\n",
       " 'welcome',\n",
       " 'wiki',\n",
       " 'utc',\n",
       " 'request',\n",
       " 'subject',\n",
       " 'let',\n",
       " 'might',\n",
       " 'believe',\n",
       " 'stop',\n",
       " 'another',\n",
       " 'however',\n",
       " 'day',\n",
       " 'part',\n",
       " 'person',\n",
       " 'free',\n",
       " 'hi',\n",
       " 'sure',\n",
       " 'feel',\n",
       " 'start',\n",
       " 'book',\n",
       " 'us',\n",
       " 'copyright',\n",
       " 'view',\n",
       " 'agree',\n",
       " 'regard',\n",
       " 'never',\n",
       " 'actually',\n",
       " 'personal',\n",
       " 'best',\n",
       " 'attack',\n",
       " 'check',\n",
       " 'provide',\n",
       " 'war',\n",
       " 'tell',\n",
       " 'continue',\n",
       " 'support',\n",
       " 'notice',\n",
       " 'anything',\n",
       " 'vandalism',\n",
       " 'hope',\n",
       " 'understand',\n",
       " 'long',\n",
       " 'move',\n",
       " 'term',\n",
       " 'opinion',\n",
       " 'review',\n",
       " 'site',\n",
       " \"'d\",\n",
       " 'already',\n",
       " 'great',\n",
       " 'explain',\n",
       " 'com',\n",
       " 'though',\n",
       " 'far',\n",
       " 'ca',\n",
       " 'example',\n",
       " 'speedy',\n",
       " 'wrong',\n",
       " 'style',\n",
       " 'contribution',\n",
       " 'text',\n",
       " 'world',\n",
       " 'number',\n",
       " 'message',\n",
       " 'title',\n",
       " 'others',\n",
       " 'appear',\n",
       " 'last',\n",
       " 'fair',\n",
       " 'e',\n",
       " 'reliable',\n",
       " 'anyone',\n",
       " 'rather',\n",
       " 'rule',\n",
       " 'nothing',\n",
       " 'different',\n",
       " 'english',\n",
       " 'must',\n",
       " 'account',\n",
       " 'template',\n",
       " 'sorry',\n",
       " 'follow',\n",
       " 'matter',\n",
       " 'suggest',\n",
       " 'non',\n",
       " 'guideline',\n",
       " \"''\",\n",
       " 'ip',\n",
       " 'group',\n",
       " 'cite',\n",
       " 'discuss',\n",
       " 'little',\n",
       " 'original',\n",
       " 'address',\n",
       " 'statement',\n",
       " 'lot',\n",
       " 'correct',\n",
       " 'material',\n",
       " 'language',\n",
       " 'top',\n",
       " 'consensus',\n",
       " 'report',\n",
       " 'simply',\n",
       " 'c',\n",
       " 'website',\n",
       " 'probably',\n",
       " 'notable',\n",
       " 'hello',\n",
       " 'criterion',\n",
       " 'date',\n",
       " 'either',\n",
       " 'else',\n",
       " 'least',\n",
       " 'etc',\n",
       " 'version',\n",
       " 'base',\n",
       " 'bad',\n",
       " 'idea',\n",
       " 'yes',\n",
       " 'www',\n",
       " 'clear',\n",
       " 'every',\n",
       " 'enough',\n",
       " 'evidence',\n",
       " 'exist',\n",
       " 'research',\n",
       " 'topic',\n",
       " 'category',\n",
       " 'medium',\n",
       " 'file',\n",
       " 'encyclopedia',\n",
       " 'quote',\n",
       " 'old',\n",
       " 'around',\n",
       " 'end',\n",
       " 'yet',\n",
       " 'clearly',\n",
       " 'bit',\n",
       " 'picture',\n",
       " 'happen',\n",
       " 'interest',\n",
       " 'pov',\n",
       " 'important',\n",
       " 'country',\n",
       " 'perhaps',\n",
       " 'whether',\n",
       " 'instead',\n",
       " 'become',\n",
       " 'lead',\n",
       " 'maybe',\n",
       " 'life',\n",
       " 'quite',\n",
       " 'real',\n",
       " 'always',\n",
       " 'org',\n",
       " 'true',\n",
       " 'american',\n",
       " 'contribute',\n",
       " 'citation',\n",
       " 'sentence',\n",
       " 'answer',\n",
       " 'refer',\n",
       " 'admin',\n",
       " 'allow',\n",
       " 'three',\n",
       " 'sign',\n",
       " 'second',\n",
       " 'line',\n",
       " 'concern',\n",
       " 'several',\n",
       " 'administrator',\n",
       " 'redirect',\n",
       " 'ever',\n",
       " 'high',\n",
       " 'school',\n",
       " 'argument',\n",
       " 'current',\n",
       " 'project',\n",
       " 'live',\n",
       " 'general',\n",
       " 'action',\n",
       " 'game',\n",
       " 'present',\n",
       " 'dispute',\n",
       " 'oh',\n",
       " '``',\n",
       " 'summary',\n",
       " 'b',\n",
       " 'result',\n",
       " 'common',\n",
       " 'wish',\n",
       " 'guy',\n",
       " 'main',\n",
       " 'possible',\n",
       " 'kind',\n",
       " 'ban',\n",
       " 'decide',\n",
       " 'test',\n",
       " 'course',\n",
       " 'jpg',\n",
       " 'accept',\n",
       " 'order',\n",
       " 'type',\n",
       " 'learn',\n",
       " 'mind',\n",
       " 'upload',\n",
       " 'less',\n",
       " 'improve',\n",
       " 'member',\n",
       " 'p',\n",
       " 'u',\n",
       " 'attempt',\n",
       " 'describe',\n",
       " 'whole',\n",
       " 'play',\n",
       " 'big',\n",
       " 'community',\n",
       " 'notability',\n",
       " 'appreciate',\n",
       " 'th',\n",
       " 'contribs',\n",
       " 'form',\n",
       " 'party',\n",
       " 'position',\n",
       " 'company',\n",
       " 'sense',\n",
       " 'happy',\n",
       " 'specific',\n",
       " 'four',\n",
       " 'en',\n",
       " 'man',\n",
       " 'care',\n",
       " 'info',\n",
       " 'bias',\n",
       " 'city',\n",
       " 'ok',\n",
       " 'side',\n",
       " 'appropriate',\n",
       " 'week',\n",
       " 'entry',\n",
       " 'involve',\n",
       " 'publish',\n",
       " 'neutral',\n",
       " 'copy',\n",
       " 'news',\n",
       " 'vandalize',\n",
       " 'color',\n",
       " 'standard',\n",
       " 'fix',\n",
       " 'reply',\n",
       " 'per',\n",
       " 'meet',\n",
       " 'single',\n",
       " 'film',\n",
       " 'although',\n",
       " 'system',\n",
       " 'relevant',\n",
       " 'detail',\n",
       " 'act',\n",
       " 'recent',\n",
       " 'large',\n",
       " 'paragraph',\n",
       " 'g',\n",
       " 'anyway',\n",
       " 'search',\n",
       " 'hey',\n",
       " 'process',\n",
       " 'f',\n",
       " 'next',\n",
       " 'law',\n",
       " 'propose',\n",
       " 'official',\n",
       " 'faith',\n",
       " 'currently',\n",
       " 'record',\n",
       " 'r',\n",
       " 'bring',\n",
       " 'area',\n",
       " 'release',\n",
       " 'band',\n",
       " 'public',\n",
       " 'relate',\n",
       " 'response',\n",
       " 'love',\n",
       " 'speak',\n",
       " 'cause',\n",
       " 'full',\n",
       " 'interested',\n",
       " 'month',\n",
       " 'stay',\n",
       " 'assume',\n",
       " 'background',\n",
       " 'theory',\n",
       " 'especially',\n",
       " 'power',\n",
       " 'able',\n",
       " 'web',\n",
       " 'accord',\n",
       " 'run',\n",
       " 'within',\n",
       " 'author',\n",
       " 'require',\n",
       " 'prove',\n",
       " 'addition',\n",
       " 'archive',\n",
       " 'deal',\n",
       " 'certainly',\n",
       " 'lol',\n",
       " 'open',\n",
       " 'warning',\n",
       " 'reader',\n",
       " 'away',\n",
       " 'cover',\n",
       " 'remember',\n",
       " 'close',\n",
       " 'hard',\n",
       " 'sort',\n",
       " 'indicate',\n",
       " 'unless',\n",
       " 'government',\n",
       " 'force',\n",
       " 'today',\n",
       " 'stuff',\n",
       " 'story',\n",
       " 'sandbox',\n",
       " 'everyone',\n",
       " 'vote',\n",
       " 'therefore',\n",
       " 'pretty',\n",
       " 'hand',\n",
       " 'future',\n",
       " 'event',\n",
       " 'due',\n",
       " 'completely',\n",
       " 'later',\n",
       " 'sound',\n",
       " 'character',\n",
       " 'hear',\n",
       " 'friend',\n",
       " 'explanation',\n",
       " 'self',\n",
       " 'google',\n",
       " 'early',\n",
       " 'definition',\n",
       " 'contact',\n",
       " 'v',\n",
       " 'past',\n",
       " 'anti',\n",
       " 'description',\n",
       " 'everything',\n",
       " 'set',\n",
       " 'similar',\n",
       " 'god',\n",
       " 'conflict',\n",
       " 'hour',\n",
       " 'nice',\n",
       " 'political',\n",
       " 'ago',\n",
       " 'disagree',\n",
       " 'national',\n",
       " 'study',\n",
       " 'small',\n",
       " 'obviously',\n",
       " 'email',\n",
       " 'photo',\n",
       " 'respond',\n",
       " 'figure',\n",
       " 'five',\n",
       " 'guess',\n",
       " 'class',\n",
       " 'truth',\n",
       " 'british',\n",
       " 'watch',\n",
       " 'exactly',\n",
       " 'useful',\n",
       " 'hold',\n",
       " 'particular',\n",
       " 'false',\n",
       " 'avoid',\n",
       " 'major',\n",
       " 'united',\n",
       " 'short',\n",
       " 'cheer',\n",
       " 'log',\n",
       " 'mistake',\n",
       " 'wikiproject',\n",
       " 'wo',\n",
       " 'send',\n",
       " 'violation',\n",
       " 'miss',\n",
       " 'wonder',\n",
       " 'error',\n",
       " 'lack',\n",
       " 'status',\n",
       " 'npov',\n",
       " 'often',\n",
       " 'family',\n",
       " 'produce',\n",
       " 'username',\n",
       " 'university',\n",
       " 'lie',\n",
       " 'dont',\n",
       " 'music',\n",
       " 'whatever',\n",
       " 'knowledge',\n",
       " 'ignore',\n",
       " 'almost',\n",
       " 'update',\n",
       " 'unblock',\n",
       " 'generally',\n",
       " 'fine',\n",
       " 'stand',\n",
       " 'criticism',\n",
       " 'german',\n",
       " 'merge',\n",
       " 'suppose',\n",
       " 'recently',\n",
       " 'certain',\n",
       " 'experiment',\n",
       " 'white',\n",
       " 'otherwise',\n",
       " 'fail',\n",
       " 'aware',\n",
       " 'level',\n",
       " 'remain',\n",
       " 'likely',\n",
       " 'debate',\n",
       " 'begin',\n",
       " 'apply',\n",
       " 'couple',\n",
       " 'human',\n",
       " 'along',\n",
       " 'context',\n",
       " 'protect',\n",
       " 'science',\n",
       " 'suggestion',\n",
       " 'late',\n",
       " 'saw',\n",
       " 'child',\n",
       " 'difference',\n",
       " 'external',\n",
       " 'respect',\n",
       " 'biography',\n",
       " 'entire',\n",
       " 'team',\n",
       " 'enjoy',\n",
       " 'warn',\n",
       " 'head',\n",
       " 'indeed',\n",
       " 'admins',\n",
       " 'uk',\n",
       " 'simple',\n",
       " 'break',\n",
       " 'mr',\n",
       " 'easy',\n",
       " 'video',\n",
       " 'contain',\n",
       " 'click',\n",
       " 'actual',\n",
       " 'purpose',\n",
       " 'turn',\n",
       " 'song',\n",
       " 'argue',\n",
       " 'n',\n",
       " 'woman',\n",
       " 'soon',\n",
       " 'various',\n",
       " 'individual',\n",
       " 'thus',\n",
       " 'jew',\n",
       " 'license',\n",
       " 'third',\n",
       " 'death',\n",
       " 'john',\n",
       " 'violate',\n",
       " 'greek',\n",
       " 'effort',\n",
       " 'christian',\n",
       " 'de',\n",
       " 'organization',\n",
       " 'bear',\n",
       " 'previous',\n",
       " 'replace',\n",
       " 'obvious',\n",
       " 'attention',\n",
       " 'x',\n",
       " 'box',\n",
       " 'wait',\n",
       " 'confirm',\n",
       " 'job',\n",
       " 'black',\n",
       " 'complete',\n",
       " 'abuse',\n",
       " 'control',\n",
       " 'church',\n",
       " 'lose',\n",
       " 'automatically',\n",
       " 'oppose',\n",
       " 'choose',\n",
       " 'accuse',\n",
       " 'w',\n",
       " 'doubt',\n",
       " 'proper',\n",
       " 'space',\n",
       " 'table',\n",
       " 'manual',\n",
       " 'series',\n",
       " 'separate',\n",
       " 'field',\n",
       " 'direct',\n",
       " 'situation',\n",
       " 'internet',\n",
       " 'afd',\n",
       " 'helpful',\n",
       " 'win',\n",
       " 'map',\n",
       " 'available',\n",
       " 'together',\n",
       " 'receive',\n",
       " 'album',\n",
       " 'valid',\n",
       " 'contributor',\n",
       " 'alone',\n",
       " 'multiple',\n",
       " 'sock',\n",
       " 'fight',\n",
       " 'necessary',\n",
       " 'thought',\n",
       " 'contest',\n",
       " 'nonsense',\n",
       " 'decision',\n",
       " 'million',\n",
       " 'july',\n",
       " 'establish',\n",
       " 'following',\n",
       " 'historical',\n",
       " 'period',\n",
       " 'width',\n",
       " 'service',\n",
       " 'culture',\n",
       " 'usually',\n",
       " 'tilde',\n",
       " 'document',\n",
       " 'mark',\n",
       " 'offer',\n",
       " 'proof',\n",
       " 'editing',\n",
       " 'expect',\n",
       " 'reach',\n",
       " 'accurate',\n",
       " 'restore',\n",
       " 'south',\n",
       " 'personally',\n",
       " 'insert',\n",
       " 'experience',\n",
       " 'quality',\n",
       " 'border',\n",
       " 'india',\n",
       " 'rationale',\n",
       " 'access',\n",
       " 'upon',\n",
       " 'format',\n",
       " 'kill',\n",
       " 'feature',\n",
       " 'share',\n",
       " 'promote',\n",
       " 'effect',\n",
       " 'rest',\n",
       " 'century',\n",
       " 'low',\n",
       " 'data',\n",
       " 'eye',\n",
       " 'award',\n",
       " 'episode',\n",
       " 'august',\n",
       " 'military',\n",
       " 'legal',\n",
       " 'count',\n",
       " 'religion',\n",
       " 'nominate',\n",
       " 'court',\n",
       " 'pillar',\n",
       " 'age',\n",
       " 'primary',\n",
       " 'paper',\n",
       " 'fish',\n",
       " 'directly',\n",
       " 'serious',\n",
       " 'press',\n",
       " 'behavior',\n",
       " 'wikipedian',\n",
       " 'none',\n",
       " 'march',\n",
       " 'return',\n",
       " 'movie',\n",
       " 'fan',\n",
       " 'student',\n",
       " 'online',\n",
       " 'join',\n",
       " 'special',\n",
       " 'html',\n",
       " 'okay',\n",
       " 'raise',\n",
       " 'among',\n",
       " 'club',\n",
       " 'light',\n",
       " 'modern',\n",
       " 'population',\n",
       " 'seriously',\n",
       " 'significant',\n",
       " 'expand',\n",
       " 'specifically',\n",
       " 'apparently',\n",
       " 'jewish',\n",
       " 'incorrect',\n",
       " 'inclusion',\n",
       " 'meaning',\n",
       " 'intend',\n",
       " 'june',\n",
       " 'belong',\n",
       " 'accusation',\n",
       " 'push',\n",
       " 'except',\n",
       " 'vandal',\n",
       " 'creation',\n",
       " 'plan',\n",
       " 'tutorial',\n",
       " 'red',\n",
       " 'align',\n",
       " 'civil',\n",
       " 'forget',\n",
       " 'nomination',\n",
       " 'business',\n",
       " 'disruptive',\n",
       " 'define',\n",
       " 'pay',\n",
       " 'step',\n",
       " 'particularly',\n",
       " 'sometimes',\n",
       " 'house',\n",
       " 'january',\n",
       " 'chance',\n",
       " 'value',\n",
       " 'practice',\n",
       " 'verify',\n",
       " 'specify',\n",
       " 'tv',\n",
       " 'home',\n",
       " 'king',\n",
       " 'scientific',\n",
       " 'anonymous',\n",
       " 'body',\n",
       " 'strong',\n",
       " 'computer',\n",
       " 'possibly',\n",
       " 'px',\n",
       " 'pass',\n",
       " 'international',\n",
       " 'build',\n",
       " 'st',\n",
       " 'independent',\n",
       " 'north',\n",
       " 'confuse',\n",
       " 'represent',\n",
       " 'program',\n",
       " 'majority',\n",
       " 'k',\n",
       " 'removal',\n",
       " 'nation',\n",
       " 'outside',\n",
       " 'half',\n",
       " 'movement',\n",
       " 'die',\n",
       " 'troll',\n",
       " 'focus',\n",
       " 'april',\n",
       " 'yeah',\n",
       " 'concept',\n",
       " 'l',\n",
       " 'bother',\n",
       " 'controversy',\n",
       " 'entirely',\n",
       " 'face',\n",
       " 'america',\n",
       " 'phrase',\n",
       " 'amount',\n",
       " 'final',\n",
       " 'infobox',\n",
       " 'clean',\n",
       " 'guide',\n",
       " 'art',\n",
       " 'israel',\n",
       " 'muslim',\n",
       " 'blog',\n",
       " 'french',\n",
       " 'piece',\n",
       " 'product',\n",
       " 'interesting',\n",
       " 'inappropriate',\n",
       " 'russian',\n",
       " 'worth',\n",
       " 'admit',\n",
       " 'al',\n",
       " 'neither',\n",
       " 'repeat',\n",
       " 'serve',\n",
       " 'expert',\n",
       " 'merely',\n",
       " 'minor',\n",
       " 'irrelevant',\n",
       " 'towards',\n",
       " 'flag',\n",
       " 'hit',\n",
       " 'unfortunately',\n",
       " 'stub',\n",
       " 'ga',\n",
       " 'match',\n",
       " 'finally',\n",
       " 'object',\n",
       " 'co',\n",
       " 'stick',\n",
       " 'december',\n",
       " 'acceptable',\n",
       " 'realize',\n",
       " 'develop',\n",
       " 'advice',\n",
       " 'reflect',\n",
       " 'threat',\n",
       " 'j',\n",
       " 'indian',\n",
       " 'board',\n",
       " 'mine',\n",
       " 'totally',\n",
       " 'authority',\n",
       " 'star',\n",
       " 'absolutely',\n",
       " 'origin',\n",
       " 'rewrite',\n",
       " 'player',\n",
       " 'pro',\n",
       " 'october',\n",
       " 'importance',\n",
       " 'compare',\n",
       " 'season',\n",
       " 'identify',\n",
       " 'highly',\n",
       " 'former',\n",
       " 'verifiable',\n",
       " 'unsigned',\n",
       " 'resolve',\n",
       " 'design',\n",
       " 'longer',\n",
       " 'vertical',\n",
       " 'september',\n",
       " 'engage',\n",
       " 'insult',\n",
       " 'shall',\n",
       " 'dear',\n",
       " 'label',\n",
       " 'save',\n",
       " 'across',\n",
       " 'prevent',\n",
       " 'academic',\n",
       " 'air',\n",
       " 'barnstar',\n",
       " 'participate',\n",
       " 'region',\n",
       " 'moment',\n",
       " 'drop',\n",
       " 'november',\n",
       " 'battle',\n",
       " 'men',\n",
       " 'introduction',\n",
       " 'instance',\n",
       " 'religious',\n",
       " 'race',\n",
       " 'basic',\n",
       " 'forward',\n",
       " 'spend',\n",
       " 'letter',\n",
       " 'local',\n",
       " 'minute',\n",
       " 'despite',\n",
       " 'properly',\n",
       " 'nature',\n",
       " 'college',\n",
       " 'limit',\n",
       " 'deserve',\n",
       " 'solid',\n",
       " 'proposal',\n",
       " 'spam',\n",
       " 'incident',\n",
       " 'bot',\n",
       " 'boy',\n",
       " 'easily',\n",
       " 'european',\n",
       " 'army',\n",
       " 'rfc',\n",
       " 'february',\n",
       " 'forum',\n",
       " 'president',\n",
       " 'fall',\n",
       " 'fit',\n",
       " 'definitely',\n",
       " 'related',\n",
       " 'determine',\n",
       " 'mail',\n",
       " 'tool',\n",
       " 'poor',\n",
       " 'nobody',\n",
       " 'index',\n",
       " 'behind',\n",
       " 'cool',\n",
       " 'basis',\n",
       " 'speedily',\n",
       " 'middle',\n",
       " 'newspaper',\n",
       " 'im',\n",
       " 'difficult',\n",
       " 'controversial',\n",
       " 'prefer',\n",
       " 'david',\n",
       " 'benefit',\n",
       " 'belief',\n",
       " 'spell',\n",
       " 'cut',\n",
       " 'permission',\n",
       " 'usage',\n",
       " 'dead',\n",
       " 'code',\n",
       " 'reasonable',\n",
       " 'role',\n",
       " 'west',\n",
       " 'fill',\n",
       " 'h',\n",
       " 'conclusion',\n",
       " 'quick',\n",
       " 'popular',\n",
       " 'alternative',\n",
       " 'rr',\n",
       " 'unsourced',\n",
       " 'express',\n",
       " 'seek',\n",
       " 'approach',\n",
       " 'bottom',\n",
       " 'ref',\n",
       " 'size',\n",
       " 'protection',\n",
       " 'trouble',\n",
       " 'york',\n",
       " 'treat',\n",
       " 'police',\n",
       " 'earth',\n",
       " 'ground',\n",
       " 'blank',\n",
       " 'connection',\n",
       " 'influence',\n",
       " 'social',\n",
       " 'imply',\n",
       " 'visit',\n",
       " 'btw',\n",
       " 'writer',\n",
       " 'fully',\n",
       " 'magazine',\n",
       " 'water',\n",
       " 'willing',\n",
       " 'east',\n",
       " 'society',\n",
       " 'land',\n",
       " 'defend',\n",
       " 'manner',\n",
       " 'charge',\n",
       " 'hate',\n",
       " 'nazi',\n",
       " 'weight',\n",
       " 'negative',\n",
       " 'total',\n",
       " 'apology',\n",
       " 'dr',\n",
       " 'republic',\n",
       " 'occur',\n",
       " 'money',\n",
       " 'suspect',\n",
       " 'mostly',\n",
       " 'town',\n",
       " 'clarify',\n",
       " 'dog',\n",
       " 'artist',\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_toxic_words = not_toxic_words[1:]\n",
    "not_toxic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c84a3",
   "metadata": {},
   "source": [
    "Объединим токсичные и нетоксичные слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5617090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2999"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_words = not_toxic_words + toxic_words\n",
    "len(valid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e28e7eb",
   "metadata": {},
   "source": [
    "Преобразуем колонку token_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7694ae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_text_valid'] = df['token_text'].apply(lambda sentence: [word for word in sentence if word in valid_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4dd52ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['token_text_valid'] = df['token_text_valid'].apply(lambda sentence: ' '.join([word for word in sentence]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d3e7e",
   "metadata": {},
   "source": [
    "Найдём количество отзывов, в которых нет выбранных слов для каждой группы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19388ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[((df['token_text_valid'] == '') | (df['token_text_valid'] == ' ')) & (df['toxic'] == 0), 'toxic'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ec88c786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[((df['token_text_valid'] == '') | (df['token_text_valid'] == ' ')) & (df['toxic'] == 1), 'toxic'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1db61f3",
   "metadata": {},
   "source": [
    "Количество таких отзывов мало относительно всей таблицы, поэтому их можно убрать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86a455ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 158676 entries, 0 to 159291\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   text              158676 non-null  object\n",
      " 1   toxic             158676 non-null  int64 \n",
      " 2   token_text        158676 non-null  object\n",
      " 3   token_text_valid  158676 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 6.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.loc[~((df['token_text_valid'] == '') | (df['token_text_valid'] == ' '))]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba829f26",
   "metadata": {},
   "source": [
    "Сделаем dataframe из преобразованного текста для корректной работы метода fit моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8383abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text = pd.DataFrame(df['token_text_valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b4ad5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation edits make username fan revert n't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>match background colour 'm stick thanks talk j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man 'm really try edit war 's guy constant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ca n't make real suggestion improvement wonder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sir hero chance remember page 's</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    token_text_valid\n",
       "0  explanation edits make username fan revert n't...\n",
       "1  match background colour 'm stick thanks talk j...\n",
       "2  hey man 'm really try edit war 's guy constant...\n",
       "3  ca n't make real suggestion improvement wonder...\n",
       "4                   sir hero chance remember page 's"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7241d",
   "metadata": {},
   "source": [
    "## 3. Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3cb98",
   "metadata": {},
   "source": [
    "### 3.1 Разделение данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5802f54",
   "metadata": {},
   "source": [
    "Разделим данные на тренировочную и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dad9e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_text,\n",
    "    df['toxic'],\n",
    "    random_state = 42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5fe7e",
   "metadata": {},
   "source": [
    "### 3.2 Составление пайплайнов и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9822c8",
   "metadata": {},
   "source": [
    "Далее необходимо составить пайплайны для ML модели. Чтобы нелинейные модели работали быстро учтём дисбаланс классов и применим undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69308566",
   "metadata": {},
   "source": [
    "Составим пайплайны для моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e561063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_imb = Pipeline_imb([\n",
    "    #('sample', RandomUnderSampler()),\n",
    "    ('col_selector', ColumnSelector(cols=('token_text_valid'),drop_axis=True)),\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression(random_state = 42)\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab616144",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'model': [LinearSVC()]\n",
    "        #'model__kernel': ['linear']\n",
    "    },\n",
    "    {\n",
    "        'model': [LogisticRegression(random_state = 42)],\n",
    "        'model__C': range(1, 10)\n",
    "    },\n",
    "    {\n",
    "        'model': [DecisionTreeClassifier(random_state=42)],\n",
    "        'model__max_depth': range(2, 5),\n",
    "        'model__min_samples_split': range(2, 5),\n",
    "        'model__min_samples_leaf': range(1, 5),\n",
    "    },\n",
    "    {\n",
    "        'model': [BernoulliNB()]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d3a637f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(\n",
    "    pipeline_imb,\n",
    "    param_grid,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1',\n",
    "    cv=5,\n",
    "    error_score='raise',\n",
    "    verbose=10 #специально поставил чтобы видеть время обучения\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cee38b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 47 candidates, totalling 235 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, error_score=&#x27;raise&#x27;,\n",
       "             estimator=Pipeline(steps=[(&#x27;col_selector&#x27;,\n",
       "                                        ColumnSelector(cols=&#x27;token_text_valid&#x27;,\n",
       "                                                       drop_axis=True)),\n",
       "                                       (&#x27;vect&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;model&#x27;: [LinearSVC()]},\n",
       "                         {&#x27;model&#x27;: [LogisticRegression(random_state=42)],\n",
       "                          &#x27;model__C&#x27;: range(1, 10)},\n",
       "                         {&#x27;model&#x27;: [DecisionTreeClassifier(random_state=42)],\n",
       "                          &#x27;model__max_depth&#x27;: range(2, 5),\n",
       "                          &#x27;model__min_samples_leaf&#x27;: range(1, 5),\n",
       "                          &#x27;model__min_samples_split&#x27;: range(2, 5)},\n",
       "                         {&#x27;model&#x27;: [BernoulliNB()]}],\n",
       "             scoring=&#x27;f1&#x27;, verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5, error_score=&#x27;raise&#x27;,\n",
       "             estimator=Pipeline(steps=[(&#x27;col_selector&#x27;,\n",
       "                                        ColumnSelector(cols=&#x27;token_text_valid&#x27;,\n",
       "                                                       drop_axis=True)),\n",
       "                                       (&#x27;vect&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;model&#x27;,\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{&#x27;model&#x27;: [LinearSVC()]},\n",
       "                         {&#x27;model&#x27;: [LogisticRegression(random_state=42)],\n",
       "                          &#x27;model__C&#x27;: range(1, 10)},\n",
       "                         {&#x27;model&#x27;: [DecisionTreeClassifier(random_state=42)],\n",
       "                          &#x27;model__max_depth&#x27;: range(2, 5),\n",
       "                          &#x27;model__min_samples_leaf&#x27;: range(1, 5),\n",
       "                          &#x27;model__min_samples_split&#x27;: range(2, 5)},\n",
       "                         {&#x27;model&#x27;: [BernoulliNB()]}],\n",
       "             scoring=&#x27;f1&#x27;, verbose=10)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;col_selector&#x27;,\n",
       "                 ColumnSelector(cols=&#x27;token_text_valid&#x27;, drop_axis=True)),\n",
       "                (&#x27;vect&#x27;, TfidfVectorizer()), (&#x27;model&#x27;, LinearSVC())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">ColumnSelector</label><div class=\"sk-toggleable__content fitted\"><pre>ColumnSelector(cols=&#x27;token_text_valid&#x27;, drop_axis=True)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;TfidfVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">?<span>Documentation for TfidfVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>TfidfVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LinearSVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC()</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "             estimator=Pipeline(steps=[('col_selector',\n",
       "                                        ColumnSelector(cols='token_text_valid',\n",
       "                                                       drop_axis=True)),\n",
       "                                       ('vect', TfidfVectorizer()),\n",
       "                                       ('model',\n",
       "                                        LogisticRegression(random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'model': [LinearSVC()]},\n",
       "                         {'model': [LogisticRegression(random_state=42)],\n",
       "                          'model__C': range(1, 10)},\n",
       "                         {'model': [DecisionTreeClassifier(random_state=42)],\n",
       "                          'model__max_depth': range(2, 5),\n",
       "                          'model__min_samples_leaf': range(1, 5),\n",
       "                          'model__min_samples_split': range(2, 5)},\n",
       "                         {'model': [BernoulliNB()]}],\n",
       "             scoring='f1', verbose=10)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deba57f",
   "metadata": {},
   "source": [
    "Найдём лучшую метрику на кросс-валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4668791e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7799227720719295"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee5465",
   "metadata": {},
   "source": [
    "Найдём метрику на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc61582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d53baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "403aeb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.777333517165311"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe53a6",
   "metadata": {},
   "source": [
    "## 4. Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7035c653",
   "metadata": {},
   "source": [
    "В задаче было необходимо составить модель, которая будет отличать токсичные комментарии от нетоксичных. Для достижения этой цели были отобраны токсичные и нетоксичные слова с помощью топа самых встречающихся слов, после чего каждый комментарий был очищен от остальных слов. В качестве модели были использованы логистическая регрессия и Linear SVC. Лучшей моделью оказалась модель Linear SVC, f1 мера на кросс-валидации оказалась равна 0.777, на тестовой выборке 0.775."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 370,
    "start_time": "2025-02-06T21:53:43.740Z"
   },
   {
    "duration": 96,
    "start_time": "2025-02-06T21:53:45.241Z"
   },
   {
    "duration": 3875,
    "start_time": "2025-02-07T17:57:05.243Z"
   },
   {
    "duration": 3179,
    "start_time": "2025-02-07T17:57:11.243Z"
   },
   {
    "duration": 2425,
    "start_time": "2025-02-07T18:03:47.312Z"
   },
   {
    "duration": 31,
    "start_time": "2025-02-07T18:03:54.608Z"
   },
   {
    "duration": 19,
    "start_time": "2025-02-07T18:04:48.646Z"
   },
   {
    "duration": 20,
    "start_time": "2025-02-07T18:05:44.085Z"
   },
   {
    "duration": 20,
    "start_time": "2025-02-07T18:06:31.773Z"
   },
   {
    "duration": 19,
    "start_time": "2025-02-07T18:07:29.394Z"
   },
   {
    "duration": 2315,
    "start_time": "2025-02-07T18:08:01.235Z"
   },
   {
    "duration": 3088,
    "start_time": "2025-02-07T18:08:05.389Z"
   },
   {
    "duration": 218,
    "start_time": "2025-02-07T18:08:20.126Z"
   },
   {
    "duration": 1219,
    "start_time": "2025-02-07T18:11:15.430Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-07T18:11:32.836Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-07T18:11:38.383Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-07T18:12:28.243Z"
   },
   {
    "duration": 3295,
    "start_time": "2025-02-07T18:12:31.202Z"
   },
   {
    "duration": 1799,
    "start_time": "2025-02-07T18:12:37.803Z"
   },
   {
    "duration": 41034,
    "start_time": "2025-02-07T18:34:06.794Z"
   },
   {
    "duration": 1414,
    "start_time": "2025-02-07T18:35:08.672Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-07T18:35:11.223Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-07T18:35:53.101Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-07T18:36:47.582Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T18:37:48.448Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T18:38:00.564Z"
   },
   {
    "duration": 754,
    "start_time": "2025-02-07T18:42:48.546Z"
   },
   {
    "duration": 4165,
    "start_time": "2025-02-07T18:42:51.094Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T18:42:57.658Z"
   },
   {
    "duration": 138,
    "start_time": "2025-02-07T18:43:00.373Z"
   },
   {
    "duration": 94,
    "start_time": "2025-02-07T18:43:03.001Z"
   },
   {
    "duration": 418,
    "start_time": "2025-02-07T18:43:05.801Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T18:43:07.804Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-07T18:43:15.145Z"
   },
   {
    "duration": 36,
    "start_time": "2025-02-07T18:43:16.504Z"
   },
   {
    "duration": 918,
    "start_time": "2025-02-07T18:43:24.630Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T18:43:30.093Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T18:43:34.812Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-07T18:43:37.293Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-07T18:43:41.287Z"
   },
   {
    "duration": 190212,
    "start_time": "2025-02-07T18:43:44.394Z"
   },
   {
    "duration": 441,
    "start_time": "2025-02-07T18:46:57.907Z"
   },
   {
    "duration": 44,
    "start_time": "2025-02-07T18:47:00.950Z"
   },
   {
    "duration": 44,
    "start_time": "2025-02-07T18:47:02.832Z"
   },
   {
    "duration": 164,
    "start_time": "2025-02-07T18:47:06.399Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-07T18:48:45.865Z"
   },
   {
    "duration": 29,
    "start_time": "2025-02-07T18:48:59.260Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:00:15.668Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:00:45.648Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:00:49.166Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:00:51.092Z"
   },
   {
    "duration": 55,
    "start_time": "2025-02-07T19:00:51.655Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:02:55.547Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-07T19:03:15.228Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-07T19:04:05.880Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:04:15.942Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:04:32.869Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:04:33.744Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:04:34.409Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:04:36.694Z"
   },
   {
    "duration": 51,
    "start_time": "2025-02-07T19:04:37.399Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:05:06.544Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:05:07.134Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:05:08.129Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:05:09.714Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:05:11.131Z"
   },
   {
    "duration": 57,
    "start_time": "2025-02-07T19:05:11.585Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:05:54.669Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:05:57.770Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:01.784Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:06:02.408Z"
   },
   {
    "duration": 56,
    "start_time": "2025-02-07T19:06:03.814Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:23.270Z"
   },
   {
    "duration": 59,
    "start_time": "2025-02-07T19:06:25.849Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:31.916Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:32.359Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:33.006Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:34.422Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:35.917Z"
   },
   {
    "duration": 64,
    "start_time": "2025-02-07T19:06:36.459Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:55.893Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:06:57.622Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:06:58.366Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:06:58.877Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:07:00.475Z"
   },
   {
    "duration": 58,
    "start_time": "2025-02-07T19:07:00.945Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:07:10.058Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:07:10.555Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:07:11.155Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:07:11.500Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:07:13.458Z"
   },
   {
    "duration": 60,
    "start_time": "2025-02-07T19:07:13.885Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:08:11.226Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:08:11.700Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:08:12.312Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:08:13.664Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:08:15.385Z"
   },
   {
    "duration": 53,
    "start_time": "2025-02-07T19:08:16.463Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:10:29.010Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:10:30.410Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:10:31.045Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:10:32.181Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:10:33.654Z"
   },
   {
    "duration": 62,
    "start_time": "2025-02-07T19:10:34.387Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:10:43.298Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:11:59.021Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:12:01.052Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:12:01.479Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:12:02.401Z"
   },
   {
    "duration": 54,
    "start_time": "2025-02-07T19:12:02.875Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:12:14.824Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:12:15.485Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:12:17.045Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:12:17.599Z"
   },
   {
    "duration": 52,
    "start_time": "2025-02-07T19:12:18.143Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:14.236Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:14:14.611Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:16.576Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:14:17.088Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:17.462Z"
   },
   {
    "duration": 53,
    "start_time": "2025-02-07T19:14:18.611Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:14:44.216Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-07T19:14:44.652Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:50.713Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:51.123Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:53.539Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:54.633Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:14:55.220Z"
   },
   {
    "duration": 73,
    "start_time": "2025-02-07T19:14:56.090Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:15:06.398Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-07T19:15:11.775Z"
   },
   {
    "duration": 87,
    "start_time": "2025-02-07T19:15:19.989Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:15:25.077Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:15:26.445Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:15:26.824Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:15:28.268Z"
   },
   {
    "duration": 52,
    "start_time": "2025-02-07T19:15:28.632Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:17.355Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:19.421Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:20.878Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:21.103Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:23.203Z"
   },
   {
    "duration": 72,
    "start_time": "2025-02-07T19:16:23.743Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:45.325Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:46.350Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:46.997Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:47.924Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:16:48.507Z"
   },
   {
    "duration": 61,
    "start_time": "2025-02-07T19:16:49.632Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:21:28.054Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:21:29.496Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:21:30.303Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:21:31.733Z"
   },
   {
    "duration": 21,
    "start_time": "2025-02-07T19:21:39.458Z"
   },
   {
    "duration": 60,
    "start_time": "2025-02-07T19:24:11.641Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:24:28.796Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:24:29.963Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:24:30.974Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:24:32.625Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:24:34.372Z"
   },
   {
    "duration": 17,
    "start_time": "2025-02-07T19:24:39.723Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:25:30.545Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:25:31.746Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:25:46.203Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:25:47.428Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:25:48.798Z"
   },
   {
    "duration": 19,
    "start_time": "2025-02-07T19:25:49.281Z"
   },
   {
    "duration": 43,
    "start_time": "2025-02-07T19:25:56.352Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:26:15.053Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:26:15.262Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:26:16.176Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:26:17.524Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:26:18.710Z"
   },
   {
    "duration": 38,
    "start_time": "2025-02-07T19:26:19.266Z"
   },
   {
    "duration": 21,
    "start_time": "2025-02-07T19:32:45.628Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-07T19:33:30.302Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-07T19:34:34.292Z"
   },
   {
    "duration": 19,
    "start_time": "2025-02-07T19:34:44.112Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-07T19:37:30.559Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-07T19:37:42.714Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:39:43.288Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:39:44.011Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:39:47.425Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:39:48.862Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:39:49.434Z"
   },
   {
    "duration": 57,
    "start_time": "2025-02-07T19:39:50.434Z"
   },
   {
    "duration": 28,
    "start_time": "2025-02-07T19:40:31.643Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:40:34.712Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:40:36.273Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:40:36.475Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:40:36.842Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:40:38.171Z"
   },
   {
    "duration": 54,
    "start_time": "2025-02-07T19:40:38.562Z"
   },
   {
    "duration": 31,
    "start_time": "2025-02-07T19:41:09.437Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:41:21.334Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:41:23.040Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:41:25.016Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-07T19:41:25.321Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-07T19:41:49.657Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-07T19:42:14.748Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-07T19:42:25.652Z"
   },
   {
    "duration": 1026,
    "start_time": "2025-02-07T19:43:24.479Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-07T19:43:25.507Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-07T19:43:31.790Z"
   },
   {
    "duration": 32,
    "start_time": "2025-02-07T19:43:32.064Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:43:38.628Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:43:39.707Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:43:41.669Z"
   },
   {
    "duration": 546,
    "start_time": "2025-02-07T19:43:42.097Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-07T19:43:42.645Z"
   },
   {
    "duration": 25,
    "start_time": "2025-02-07T19:43:50.508Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-07T19:44:06.982Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:44:31.861Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-07T19:44:32.095Z"
   },
   {
    "duration": 24,
    "start_time": "2025-02-07T19:44:42.110Z"
   },
   {
    "duration": 32,
    "start_time": "2025-02-07T19:44:44.551Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-07T19:44:46.529Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-07T19:44:50.105Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:44:52.224Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:44:53.701Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:44:54.297Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:44:56.502Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-07T19:44:56.978Z"
   },
   {
    "duration": 23,
    "start_time": "2025-02-07T19:45:31.607Z"
   },
   {
    "duration": 47,
    "start_time": "2025-02-07T19:45:35.635Z"
   },
   {
    "duration": 24,
    "start_time": "2025-02-07T19:46:51.287Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-07T19:46:54.212Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-07T19:46:55.893Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-07T19:46:57.489Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:46:59.313Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:47:00.159Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:47:02.428Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-07T19:47:04.154Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:49:59.188Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:50:00.929Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:50:01.333Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-07T19:50:01.629Z"
   },
   {
    "duration": 25,
    "start_time": "2025-02-07T19:51:12.883Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:51:18.529Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-07T19:51:18.805Z"
   },
   {
    "duration": 2,
    "start_time": "2025-02-07T19:51:20.322Z"
   },
   {
    "duration": 50,
    "start_time": "2025-02-07T19:51:20.661Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-07T19:51:32.815Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-07T19:51:34.849Z"
   },
   {
    "duration": 8251,
    "start_time": "2025-02-08T09:52:03.849Z"
   },
   {
    "duration": 3619,
    "start_time": "2025-02-08T09:52:12.102Z"
   },
   {
    "duration": 962,
    "start_time": "2025-02-08T09:52:15.723Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-08T09:52:16.687Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-08T09:52:16.701Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T09:52:16.720Z"
   },
   {
    "duration": 3854,
    "start_time": "2025-02-08T09:52:16.726Z"
   },
   {
    "duration": 2011,
    "start_time": "2025-02-08T09:52:20.581Z"
   },
   {
    "duration": 48161,
    "start_time": "2025-02-08T09:52:22.594Z"
   },
   {
    "duration": 1569,
    "start_time": "2025-02-08T09:53:10.757Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-08T09:53:12.329Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-08T09:53:12.342Z"
   },
   {
    "duration": 935,
    "start_time": "2025-02-08T09:53:12.355Z"
   },
   {
    "duration": 5316,
    "start_time": "2025-02-08T09:53:13.293Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T09:53:18.611Z"
   },
   {
    "duration": 173,
    "start_time": "2025-02-08T09:53:18.617Z"
   },
   {
    "duration": 121,
    "start_time": "2025-02-08T09:53:18.792Z"
   },
   {
    "duration": 515,
    "start_time": "2025-02-08T09:53:18.915Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T09:53:19.432Z"
   },
   {
    "duration": 21,
    "start_time": "2025-02-08T09:53:19.438Z"
   },
   {
    "duration": 45,
    "start_time": "2025-02-08T09:53:19.461Z"
   },
   {
    "duration": 1098,
    "start_time": "2025-02-08T09:53:19.508Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T09:53:20.608Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T09:53:20.614Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-08T09:53:20.619Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T09:53:20.636Z"
   },
   {
    "duration": 221331,
    "start_time": "2025-02-08T09:53:20.642Z"
   },
   {
    "duration": 465,
    "start_time": "2025-02-08T09:57:01.974Z"
   },
   {
    "duration": 53,
    "start_time": "2025-02-08T09:57:02.441Z"
   },
   {
    "duration": 56,
    "start_time": "2025-02-08T09:57:02.496Z"
   },
   {
    "duration": 201,
    "start_time": "2025-02-08T09:57:02.554Z"
   },
   {
    "duration": 16,
    "start_time": "2025-02-08T09:57:02.757Z"
   },
   {
    "duration": 42,
    "start_time": "2025-02-08T09:57:02.775Z"
   },
   {
    "duration": 31,
    "start_time": "2025-02-08T09:57:02.820Z"
   },
   {
    "duration": 39,
    "start_time": "2025-02-08T09:57:02.853Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-08T09:57:02.894Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-08T09:57:02.905Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-08T09:57:02.916Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-08T09:57:02.926Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T09:57:02.934Z"
   },
   {
    "duration": 259,
    "start_time": "2025-02-08T09:57:02.974Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T09:57:03.235Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T09:57:03.237Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T09:57:03.238Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T09:57:03.240Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-08T09:59:50.309Z"
   },
   {
    "duration": 16,
    "start_time": "2025-02-08T10:01:23.889Z"
   },
   {
    "duration": 3737,
    "start_time": "2025-02-08T10:02:23.949Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-08T10:02:30.604Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:02:39.434Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:02:40.995Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:02:42.178Z"
   },
   {
    "duration": 986,
    "start_time": "2025-02-08T10:02:42.716Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:03:04.107Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:03:04.433Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:03:05.965Z"
   },
   {
    "duration": 56498,
    "start_time": "2025-02-08T10:03:06.963Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:04:09.429Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:27:08.700Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:27:25.488Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:27:51.915Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:27:54.262Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:27:55.949Z"
   },
   {
    "duration": 50,
    "start_time": "2025-02-08T10:27:56.342Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:28:27.941Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:28:28.333Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T10:28:28.610Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T10:28:30.127Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:28:30.731Z"
   },
   {
    "duration": 44,
    "start_time": "2025-02-08T10:28:33.914Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:28:53.689Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:28:54.889Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:28:55.085Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T10:28:56.215Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:28:56.862Z"
   },
   {
    "duration": 45,
    "start_time": "2025-02-08T10:28:57.891Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:29:14.896Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-08T10:29:15.169Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T10:29:15.472Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:29:16.699Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:29:17.247Z"
   },
   {
    "duration": 65,
    "start_time": "2025-02-08T10:29:18.121Z"
   },
   {
    "duration": 8,
    "start_time": "2025-02-08T10:33:07.123Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:33:55.163Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:34:01.062Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:34:08.406Z"
   },
   {
    "duration": 62356,
    "start_time": "2025-02-08T10:34:09.145Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:35:17.608Z"
   },
   {
    "duration": 77,
    "start_time": "2025-02-08T10:36:32.461Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T10:36:40.068Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:36:41.924Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:36:44.235Z"
   },
   {
    "duration": 118174,
    "start_time": "2025-02-08T10:36:44.768Z"
   },
   {
    "duration": 47,
    "start_time": "2025-02-08T10:41:38.367Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:41:43.102Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T10:41:43.813Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T10:41:45.936Z"
   },
   {
    "duration": 27498,
    "start_time": "2025-02-08T10:41:46.892Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-08T10:43:08.569Z"
   },
   {
    "duration": 6,
    "start_time": "2025-02-08T10:43:10.485Z"
   },
   {
    "duration": 2793,
    "start_time": "2025-02-08T11:21:05.933Z"
   },
   {
    "duration": 2903,
    "start_time": "2025-02-08T11:21:08.728Z"
   },
   {
    "duration": 3350,
    "start_time": "2025-02-08T11:21:11.634Z"
   },
   {
    "duration": 1018,
    "start_time": "2025-02-08T11:21:14.986Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-08T11:21:16.007Z"
   },
   {
    "duration": 16,
    "start_time": "2025-02-08T11:21:16.021Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T11:21:16.039Z"
   },
   {
    "duration": 4154,
    "start_time": "2025-02-08T11:21:16.045Z"
   },
   {
    "duration": 2109,
    "start_time": "2025-02-08T11:21:20.201Z"
   },
   {
    "duration": 50128,
    "start_time": "2025-02-08T11:21:22.312Z"
   },
   {
    "duration": 1610,
    "start_time": "2025-02-08T11:22:12.442Z"
   },
   {
    "duration": 21,
    "start_time": "2025-02-08T11:22:14.054Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-08T11:22:14.088Z"
   },
   {
    "duration": 1394,
    "start_time": "2025-02-08T11:22:14.098Z"
   },
   {
    "duration": 5496,
    "start_time": "2025-02-08T11:22:15.493Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T11:22:20.990Z"
   },
   {
    "duration": 172,
    "start_time": "2025-02-08T11:22:20.996Z"
   },
   {
    "duration": 142,
    "start_time": "2025-02-08T11:22:21.170Z"
   },
   {
    "duration": 469,
    "start_time": "2025-02-08T11:22:21.314Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T11:22:21.784Z"
   },
   {
    "duration": 34,
    "start_time": "2025-02-08T11:22:21.791Z"
   },
   {
    "duration": 59,
    "start_time": "2025-02-08T11:22:21.826Z"
   },
   {
    "duration": 1143,
    "start_time": "2025-02-08T11:22:21.887Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T11:22:23.031Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T11:22:23.037Z"
   },
   {
    "duration": 42,
    "start_time": "2025-02-08T11:22:23.042Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-08T11:22:23.086Z"
   },
   {
    "duration": 237359,
    "start_time": "2025-02-08T11:22:23.101Z"
   },
   {
    "duration": 480,
    "start_time": "2025-02-08T11:26:20.462Z"
   },
   {
    "duration": 51,
    "start_time": "2025-02-08T11:26:20.943Z"
   },
   {
    "duration": 57,
    "start_time": "2025-02-08T11:26:20.996Z"
   },
   {
    "duration": 209,
    "start_time": "2025-02-08T11:26:21.056Z"
   },
   {
    "duration": 10,
    "start_time": "2025-02-08T11:26:21.271Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-08T11:26:21.284Z"
   },
   {
    "duration": 39,
    "start_time": "2025-02-08T11:26:21.304Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T11:26:21.345Z"
   },
   {
    "duration": 31,
    "start_time": "2025-02-08T11:26:21.351Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T11:26:21.384Z"
   },
   {
    "duration": 1649763,
    "start_time": "2025-02-08T11:26:21.390Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-08T11:53:51.155Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T11:53:51.174Z"
   },
   {
    "duration": 69653,
    "start_time": "2025-02-08T11:53:51.181Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-08T11:55:00.836Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T11:56:14.394Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T11:56:16.634Z"
   },
   {
    "duration": 252401,
    "start_time": "2025-02-08T11:56:18.548Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T12:00:34.921Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T12:00:48.811Z"
   },
   {
    "duration": 864,
    "start_time": "2025-02-08T12:00:49.273Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-08T12:00:50.140Z"
   },
   {
    "duration": 2843,
    "start_time": "2025-02-08T12:04:48.314Z"
   },
   {
    "duration": 2818,
    "start_time": "2025-02-08T12:04:51.161Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-08T12:05:30.966Z"
   },
   {
    "duration": 2774,
    "start_time": "2025-02-08T12:06:25.706Z"
   },
   {
    "duration": 2823,
    "start_time": "2025-02-08T12:06:28.483Z"
   },
   {
    "duration": 3140,
    "start_time": "2025-02-08T12:06:31.308Z"
   },
   {
    "duration": 961,
    "start_time": "2025-02-08T12:06:34.450Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-08T12:06:35.413Z"
   },
   {
    "duration": 16,
    "start_time": "2025-02-08T12:06:35.426Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-08T12:06:35.444Z"
   },
   {
    "duration": 3853,
    "start_time": "2025-02-08T12:06:35.479Z"
   },
   {
    "duration": 2014,
    "start_time": "2025-02-08T12:06:39.333Z"
   },
   {
    "duration": 46594,
    "start_time": "2025-02-08T12:06:41.349Z"
   },
   {
    "duration": 1611,
    "start_time": "2025-02-08T12:07:27.944Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-08T12:07:29.557Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-08T12:07:29.577Z"
   },
   {
    "duration": 955,
    "start_time": "2025-02-08T12:07:29.594Z"
   },
   {
    "duration": 5149,
    "start_time": "2025-02-08T12:07:30.551Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T12:07:35.701Z"
   },
   {
    "duration": 167,
    "start_time": "2025-02-08T12:07:35.707Z"
   },
   {
    "duration": 113,
    "start_time": "2025-02-08T12:07:35.875Z"
   },
   {
    "duration": 442,
    "start_time": "2025-02-08T12:07:35.990Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T12:07:36.433Z"
   },
   {
    "duration": 13,
    "start_time": "2025-02-08T12:07:36.439Z"
   },
   {
    "duration": 53,
    "start_time": "2025-02-08T12:07:36.454Z"
   },
   {
    "duration": 1095,
    "start_time": "2025-02-08T12:07:36.509Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T12:07:37.606Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-08T12:07:37.612Z"
   },
   {
    "duration": 22,
    "start_time": "2025-02-08T12:07:37.623Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-08T12:07:37.646Z"
   },
   {
    "duration": 222129,
    "start_time": "2025-02-08T12:07:37.653Z"
   },
   {
    "duration": 466,
    "start_time": "2025-02-08T12:11:19.784Z"
   },
   {
    "duration": 51,
    "start_time": "2025-02-08T12:11:20.251Z"
   },
   {
    "duration": 47,
    "start_time": "2025-02-08T12:11:20.304Z"
   },
   {
    "duration": 200,
    "start_time": "2025-02-08T12:11:20.352Z"
   },
   {
    "duration": 7,
    "start_time": "2025-02-08T12:11:20.554Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-08T12:11:20.563Z"
   },
   {
    "duration": 35,
    "start_time": "2025-02-08T12:11:20.578Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T12:11:20.615Z"
   },
   {
    "duration": 65,
    "start_time": "2025-02-08T12:11:20.620Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T12:11:20.686Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T12:11:20.688Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T12:11:20.689Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T12:11:20.691Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T12:11:20.692Z"
   },
   {
    "duration": 0,
    "start_time": "2025-02-08T12:11:20.693Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T12:11:49.235Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T12:11:51.589Z"
   },
   {
    "duration": 252734,
    "start_time": "2025-02-08T12:11:52.979Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T12:16:13.001Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T12:16:22.199Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T12:16:23.499Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T12:16:26.003Z"
   },
   {
    "duration": 812448,
    "start_time": "2025-02-08T12:16:27.200Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-08T12:30:37.906Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-08T12:30:46.222Z"
   },
   {
    "duration": 853,
    "start_time": "2025-02-08T12:30:47.069Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-08T12:30:49.214Z"
   },
   {
    "duration": 7436,
    "start_time": "2025-02-09T04:00:11.203Z"
   },
   {
    "duration": 3245,
    "start_time": "2025-02-09T04:00:21.236Z"
   },
   {
    "duration": 3174,
    "start_time": "2025-02-09T04:00:26.565Z"
   },
   {
    "duration": 1383,
    "start_time": "2025-02-09T04:00:33.292Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-09T04:00:36.747Z"
   },
   {
    "duration": 14,
    "start_time": "2025-02-09T04:00:40.268Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-09T04:00:42.821Z"
   },
   {
    "duration": 3491,
    "start_time": "2025-02-09T04:00:45.108Z"
   },
   {
    "duration": 1736,
    "start_time": "2025-02-09T04:00:50.687Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-09T04:08:28.695Z"
   },
   {
    "duration": 486,
    "start_time": "2025-02-09T04:11:47.656Z"
   },
   {
    "duration": 460,
    "start_time": "2025-02-09T04:20:27.560Z"
   },
   {
    "duration": 2577,
    "start_time": "2025-02-09T04:21:57.327Z"
   },
   {
    "duration": 2623,
    "start_time": "2025-02-09T04:21:59.906Z"
   },
   {
    "duration": 3354,
    "start_time": "2025-02-09T04:22:02.531Z"
   },
   {
    "duration": 873,
    "start_time": "2025-02-09T04:22:05.887Z"
   },
   {
    "duration": 12,
    "start_time": "2025-02-09T04:22:07.247Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-09T04:22:10.452Z"
   },
   {
    "duration": 4,
    "start_time": "2025-02-09T04:22:13.492Z"
   },
   {
    "duration": 3598,
    "start_time": "2025-02-09T04:22:16.578Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-09T04:22:21.549Z"
   },
   {
    "duration": 479209,
    "start_time": "2025-02-09T04:23:00.203Z"
   },
   {
    "duration": 175,
    "start_time": "2025-02-09T04:33:12.407Z"
   },
   {
    "duration": 315,
    "start_time": "2025-02-09T04:33:46.026Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-09T04:34:39.496Z"
   },
   {
    "duration": 9966,
    "start_time": "2025-02-09T04:34:50.755Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-09T04:35:28.684Z"
   },
   {
    "duration": 33,
    "start_time": "2025-02-09T04:38:48.162Z"
   },
   {
    "duration": 2660,
    "start_time": "2025-02-09T04:41:20.804Z"
   },
   {
    "duration": 2560,
    "start_time": "2025-02-09T04:41:24.671Z"
   },
   {
    "duration": 4234,
    "start_time": "2025-02-09T04:43:14.062Z"
   },
   {
    "duration": 5093,
    "start_time": "2025-02-09T04:43:58.401Z"
   },
   {
    "duration": 198,
    "start_time": "2025-02-09T04:44:05.674Z"
   },
   {
    "duration": 2502,
    "start_time": "2025-02-09T04:44:48.663Z"
   },
   {
    "duration": 29,
    "start_time": "2025-02-09T04:44:53.494Z"
   },
   {
    "duration": 2504,
    "start_time": "2025-02-09T04:44:57.868Z"
   },
   {
    "duration": 28,
    "start_time": "2025-02-09T04:45:02.607Z"
   },
   {
    "duration": 18,
    "start_time": "2025-02-09T04:45:38.874Z"
   },
   {
    "duration": 2835,
    "start_time": "2025-02-09T04:46:08.761Z"
   },
   {
    "duration": 39,
    "start_time": "2025-02-09T04:46:13.892Z"
   },
   {
    "duration": 9,
    "start_time": "2025-02-09T04:46:45.899Z"
   },
   {
    "duration": 1444,
    "start_time": "2025-02-09T04:46:53.053Z"
   },
   {
    "duration": 11,
    "start_time": "2025-02-09T04:46:55.572Z"
   },
   {
    "duration": 15,
    "start_time": "2025-02-09T04:46:58.099Z"
   },
   {
    "duration": 5,
    "start_time": "2025-02-09T04:47:00.557Z"
   },
   {
    "duration": 3447,
    "start_time": "2025-02-09T04:47:03.155Z"
   },
   {
    "duration": 3,
    "start_time": "2025-02-09T04:47:09.348Z"
   },
   {
    "duration": 703904,
    "start_time": "2025-02-09T04:47:48.861Z"
   },
   {
    "duration": 107269,
    "start_time": "2025-02-09T04:59:54.279Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.45px",
    "left": "1109px",
    "right": "20px",
    "top": "341px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
